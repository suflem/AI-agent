# skills/notebooklm_connector.py
# NotebookLM å…¼å®¹å±‚ï¼šå…ˆç”¨æœ¬åœ° KB + å¤–éƒ¨æ¨¡å‹å®ç° sync/ask/digest æ¥å£

import os
import re
import json
import time
import hashlib
import urllib.parse

from .registry import register
from .path_safety import guard_path

NOTEBOOKLM_STATE_REL = "data/notebooklm/notebooks.json"
NOTEBOOKLM_CACHE_REL = "data/notebooklm/cache"
NOTEBOOKLM_API_KEY_ENV = "NOTEBOOKLM_API_KEY"
NOTEBOOKLM_BASE_URL_ENV = "NOTEBOOKLM_BASE_URL"


notebooklm_connector_schema = {
    "type": "function",
    "function": {
        "name": "notebooklm_connector",
        "description": (
            "NotebookLM å…¼å®¹æ¥å£ã€‚æ”¯æŒ sync_sources / ask / digest / statusã€‚"
            "å½“å‰ä½¿ç”¨æœ¬åœ°çŸ¥è¯†åº“ + å¤–éƒ¨æ¨¡å‹ä½œä¸ºæ›¿ä»£å®ç°ï¼Œå¹¶é¢„ç•™å®˜æ–¹ API å˜é‡ä½ç½®ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "action": {
                    "type": "string",
                    "description": "æ“ä½œ: sync_sources(åŒæ­¥æº), ask(æé—®), digest(æ‘˜è¦), status(çŠ¶æ€)",
                },
                "notebook_id": {
                    "type": "string",
                    "description": "ç¬”è®°æœ¬ IDï¼ˆå­—æ¯/æ•°å­—/_/-ï¼Œé»˜è®¤ defaultï¼‰",
                },
                "notebook_name": {
                    "type": "string",
                    "description": "ç¬”è®°æœ¬åç§°ï¼ˆsync_sources æ—¶å¯é€‰ï¼‰",
                },
                "local_paths": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "æœ¬åœ°æ–‡ä»¶/ç›®å½•è·¯å¾„åˆ—è¡¨ï¼ˆsync_sourcesï¼‰",
                },
                "urls": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ç½‘é¡µæ¥æº URL åˆ—è¡¨ï¼ˆsync_sourcesï¼‰",
                },
                "question": {
                    "type": "string",
                    "description": "æé—®å†…å®¹ï¼ˆaskï¼‰",
                },
                "digest_type": {
                    "type": "string",
                    "description": "æ‘˜è¦ç±»å‹: briefing/highlights/analysisï¼ˆdigestï¼‰",
                },
                "provider": {
                    "type": "string",
                    "description": "å¤–éƒ¨æ¨¡å‹æä¾›å•†ï¼ˆé»˜è®¤ kimiï¼‰",
                },
                "top_k": {
                    "type": "integer",
                    "description": "æ£€ç´¢ç‰‡æ®µæ¡æ•°ï¼ˆé»˜è®¤ 6ï¼‰",
                },
            },
            "required": ["action"],
        },
    },
}


def _normalize_notebook_id(notebook_id: str):
    value = (notebook_id or "default").strip().lower()
    if not re.fullmatch(r"[a-z0-9_-]{1,64}", value):
        return None, "âŒ notebook_id åªå…è®¸å°å†™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’ŒçŸ­æ¨ªçº¿ï¼Œé•¿åº¦ 1-64"
    return value, None


def _validate_source_url(url: str):
    try:
        parsed = urllib.parse.urlparse(url)
    except Exception:
        return "âŒ URL è§£æå¤±è´¥"

    if parsed.scheme not in ("http", "https"):
        return "âŒ ä»…æ”¯æŒ http/https URL"

    host = (parsed.hostname or "").strip().lower()
    if not host:
        return "âŒ URL ç¼ºå°‘ä¸»æœºå"

    if host in {"localhost", "127.0.0.1", "0.0.0.0", "::1"}:
        return "âŒ å®‰å…¨æ‹¦æˆªï¼šä¸å…è®¸ä½¿ç”¨æœ¬æœºåœ°å€ä½œä¸ºæ¥æº"

    return None


def _load_state():
    state_obj, err = guard_path(NOTEBOOKLM_STATE_REL, must_exist=False, for_write=True)
    if err:
        raise RuntimeError(err)

    if not state_obj.parent.exists():
        state_obj.parent.mkdir(parents=True, exist_ok=True)

    if state_obj.exists():
        with open(state_obj, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, dict):
                return data
    return {"notebooks": {}}


def _save_state(state):
    state_obj, err = guard_path(NOTEBOOKLM_STATE_REL, must_exist=False, for_write=True)
    if err:
        raise RuntimeError(err)

    if not state_obj.parent.exists():
        state_obj.parent.mkdir(parents=True, exist_ok=True)

    with open(state_obj, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)


def _cache_file_for_url(notebook_id: str, url: str):
    digest = hashlib.sha1(url.encode("utf-8")).hexdigest()
    rel = f"{NOTEBOOKLM_CACHE_REL}/{notebook_id}/{digest}.txt"
    cache_obj, err = guard_path(rel, must_exist=False, for_write=True)
    if err:
        raise RuntimeError(err)

    if not cache_obj.parent.exists():
        cache_obj.parent.mkdir(parents=True, exist_ok=True)

    return cache_obj


def _merge_sources(existing, incoming):
    source_map = {}

    for item in existing:
        key = item.get("source_url") or item.get("path")
        if key:
            source_map[key] = item

    for item in incoming:
        key = item.get("source_url") or item.get("path")
        if key:
            source_map[key] = item

    return sorted(source_map.values(), key=lambda x: (x.get("type", ""), x.get("path", ""), x.get("source_url", "")))


@register(notebooklm_connector_schema)
def notebooklm_connector(
    action: str,
    notebook_id: str = "default",
    notebook_name: str = "",
    local_paths: list = None,
    urls: list = None,
    question: str = "",
    digest_type: str = "briefing",
    provider: str = "kimi",
    top_k: int = 6,
):
    try:
        action = (action or "").strip().lower()
        notebook_id, notebook_id_err = _normalize_notebook_id(notebook_id)
        if notebook_id_err:
            return notebook_id_err

        local_paths = local_paths or []
        urls = urls or []
        if not isinstance(local_paths, list):
            return "âŒ local_paths å¿…é¡»æ˜¯æ•°ç»„"
        if not isinstance(urls, list):
            return "âŒ urls å¿…é¡»æ˜¯æ•°ç»„"

        state = _load_state()
        notebooks = state.setdefault("notebooks", {})

        if action == "status":
            if not notebooks:
                return (
                    "ğŸ““ NotebookLM å…¼å®¹å±‚çŠ¶æ€ï¼šæš‚æ— ç¬”è®°æœ¬ã€‚\n"
                    f"API Key å˜é‡: {NOTEBOOKLM_API_KEY_ENV}\n"
                    f"Base URL å˜é‡: {NOTEBOOKLM_BASE_URL_ENV}"
                )

            lines = ["ğŸ““ NotebookLM å…¼å®¹å±‚çŠ¶æ€:\n"]
            for nid in sorted(notebooks.keys()):
                nb = notebooks[nid]
                lines.append(f"  - {nid} ({nb.get('name', nid)})")
                lines.append(f"    æ¥æºæ•°: {len(nb.get('sources', []))}")
                lines.append(f"    æ›´æ–°æ—¶é—´: {nb.get('updated_at', '-')}")

            lines.append("")
            lines.append(f"ğŸ’¡ é¢„ç•™å®˜æ–¹ API å˜é‡: {NOTEBOOKLM_API_KEY_ENV}, {NOTEBOOKLM_BASE_URL_ENV}")
            return "\n".join(lines)

        if action == "sync_sources":
            notebook = notebooks.setdefault(notebook_id, {})
            notebook["name"] = (notebook_name or notebook.get("name") or notebook_id).strip()
            notebook.setdefault("sources", [])

            incoming_sources = []
            errors = []

            for p in local_paths:
                p_obj, err = guard_path(str(p), must_exist=True, for_write=False)
                if err:
                    errors.append(f"æœ¬åœ°è·¯å¾„ {p}: {err}")
                    continue

                incoming_sources.append({
                    "type": "local",
                    "path": str(p_obj),
                    "added_at": time.strftime("%Y-%m-%d %H:%M"),
                })

            if urls:
                from .web_tools import fetch_url

            for url in urls:
                url = str(url).strip()
                url_err = _validate_source_url(url)
                if url_err:
                    errors.append(f"URL {url}: {url_err}")
                    continue

                fetched = fetch_url(url=url, max_length=18000)
                if not isinstance(fetched, str) or fetched.startswith("âŒ"):
                    errors.append(f"URL {url}: æ‹‰å–å¤±è´¥ -> {fetched}")
                    continue

                try:
                    cache_file = _cache_file_for_url(notebook_id, url)
                    with open(cache_file, "w", encoding="utf-8") as f:
                        f.write(fetched)

                    incoming_sources.append({
                        "type": "url_cache",
                        "path": str(cache_file),
                        "source_url": url,
                        "added_at": time.strftime("%Y-%m-%d %H:%M"),
                    })
                except Exception as e:
                    errors.append(f"URL {url}: ç¼“å­˜å¤±è´¥ -> {e}")

            if not incoming_sources and not notebook.get("sources"):
                return "âŒ æ²¡æœ‰å¯åŒæ­¥çš„æœ‰æ•ˆæ¥æº"

            notebook["sources"] = _merge_sources(notebook.get("sources", []), incoming_sources)
            notebook["updated_at"] = time.strftime("%Y-%m-%d %H:%M")

            kb_name = f"notebooklm_{notebook_id}"
            kb_results = []
            kb_errors = []

            from .knowledge_tools import kb_build

            for source in notebook.get("sources", []):
                source_path = source.get("path", "")
                if not source_path:
                    continue
                result = kb_build(kb_name=kb_name, source_path=source_path, chunk_size=700)
                if isinstance(result, str) and result.startswith("âœ…"):
                    kb_results.append(source_path)
                else:
                    kb_errors.append(f"{source_path}: {result}")

            _save_state(state)

            lines = [
                f"âœ… Notebook '{notebook_id}' å·²åŒæ­¥ã€‚",
                f"  åç§°: {notebook.get('name', notebook_id)}",
                f"  æ¥æºæ€»æ•°: {len(notebook.get('sources', []))}",
                f"  æœ¬æ¬¡æ–°å¢æ¥æº: {len(incoming_sources)}",
                f"  KB åŒæ­¥æˆåŠŸ: {len(kb_results)}",
            ]
            if errors:
                lines.append(f"  âš ï¸ æ¥æºå¼‚å¸¸: {len(errors)}")
                lines.extend([f"    - {e}" for e in errors[:8]])
            if kb_errors:
                lines.append(f"  âš ï¸ KB åŒæ­¥å¼‚å¸¸: {len(kb_errors)}")
                lines.extend([f"    - {e}" for e in kb_errors[:8]])

            lines.append(f"  å…¼å®¹ KB åç§°: {kb_name}")
            lines.append(f"  é¢„ç•™å®˜æ–¹ API å˜é‡: {NOTEBOOKLM_API_KEY_ENV}, {NOTEBOOKLM_BASE_URL_ENV}")
            return "\n".join(lines)

        if action == "ask":
            if not question.strip():
                return "âŒ question ä¸èƒ½ä¸ºç©º"

            notebook = notebooks.get(notebook_id)
            if not notebook:
                return f"âŒ notebook ä¸å­˜åœ¨: {notebook_id}ã€‚è¯·å…ˆ sync_sources"

            top_k = max(1, min(int(top_k) if top_k else 6, 12))
            kb_name = f"notebooklm_{notebook_id}"

            from .knowledge_tools import kb_query

            context = kb_query(kb_name=kb_name, query=question, top_k=top_k)
            if isinstance(context, str) and context.startswith("âŒ"):
                return f"âš ï¸ æ£€ç´¢å¤±è´¥ï¼Œè¯·å…ˆåŒæ­¥æ¥æºã€‚\n{context}"

            from .external_ai import call_ai

            answer = call_ai(
                prompt=(
                    f"Notebook åç§°: {notebook.get('name', notebook_id)}\n"
                    f"ç”¨æˆ·é—®é¢˜: {question}\n\n"
                    f"ä»¥ä¸‹æ˜¯æ£€ç´¢åˆ°çš„ç›¸å…³ç‰‡æ®µ:\n{context}\n\n"
                    "è¯·åŸºäºç‰‡æ®µå›ç­”ï¼šå…ˆç»™ç»“è®ºï¼Œå†ç»™è¯æ®ç‚¹ï¼›è‹¥è¯æ®ä¸è¶³è¦æ˜ç¡®è¯´æ˜ã€‚"
                ),
                provider=provider,
                system_prompt="ä½ æ˜¯ NotebookLM é£æ ¼åŠ©æ‰‹ï¼Œå›ç­”éœ€å¯è¿½æº¯åˆ°ç»™å®šç‰‡æ®µã€‚",
                temperature=0.3,
                max_tokens=4096,
            )
            return f"ğŸ““ Notebook Ask ({notebook_id})\n{answer}"

        if action == "digest":
            notebook = notebooks.get(notebook_id)
            if not notebook:
                return f"âŒ notebook ä¸å­˜åœ¨: {notebook_id}ã€‚è¯·å…ˆ sync_sources"

            top_k = max(1, min(int(top_k) if top_k else 6, 12))
            kb_name = f"notebooklm_{notebook_id}"

            from .knowledge_tools import kb_query

            query = "è¯·æå–å…³é”®ä¸»é¢˜ã€æ ¸å¿ƒäº‹å®ã€é£é™©ç‚¹ä¸åç»­è¡ŒåŠ¨"
            context = kb_query(kb_name=kb_name, query=query, top_k=top_k)
            if isinstance(context, str) and context.startswith("âŒ"):
                return f"âš ï¸ æ£€ç´¢å¤±è´¥ï¼Œè¯·å…ˆåŒæ­¥æ¥æºã€‚\n{context}"

            prompts = {
                "briefing": "ç”Ÿæˆç®€æ´æ‘˜è¦ï¼ˆ3-5 æ¡ï¼‰ã€‚",
                "highlights": "æå–æœ€é‡è¦çš„ 5 ä¸ªäº®ç‚¹å¹¶è¯´æ˜åŸå› ã€‚",
                "analysis": "åšç»“æ„åŒ–åˆ†æï¼šä¸»é¢˜ã€è¯æ®ã€é£é™©ã€å»ºè®®è¡ŒåŠ¨ã€‚",
            }
            digest_instruction = prompts.get((digest_type or "briefing").lower(), prompts["briefing"])

            from .external_ai import call_ai

            digest = call_ai(
                prompt=(
                    f"Notebook åç§°: {notebook.get('name', notebook_id)}\n"
                    f"ä»»åŠ¡: {digest_instruction}\n\n"
                    f"æ£€ç´¢ç‰‡æ®µ:\n{context}"
                ),
                provider=provider,
                system_prompt="ä½ æ˜¯ NotebookLM é£æ ¼æ‘˜è¦åŠ©æ‰‹ï¼Œå¿…é¡»åŸºäºç»™å®šç‰‡æ®µè¾“å‡ºã€‚",
                temperature=0.4,
                max_tokens=4096,
            )
            return f"ğŸ““ Notebook Digest ({notebook_id})\n{digest}"

        return "âŒ æœªçŸ¥ actionã€‚æ”¯æŒ: sync_sources, ask, digest, status"

    except Exception as e:
        return f"âŒ notebooklm_connector æ‰§è¡Œå¤±è´¥: {e}"
