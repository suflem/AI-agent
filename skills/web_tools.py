# skills/web_tools.py
# ç½‘ç»œå·¥å…·ï¼šç½‘é¡µæŠ“å–ã€URL å†…å®¹æå–ã€ç½‘ç»œæœç´¢æ¥å£

import os
import re
import json
import html as html_lib
import socket
import ipaddress
import urllib.parse
from pathlib import Path
from .registry import register


# ==========================================
# 1. æŠ“å–ç½‘é¡µå†…å®¹
# ==========================================
fetch_url_schema = {
    "type": "function",
    "function": {
        "name": "fetch_url",
        "description": (
            "æŠ“å–æŒ‡å®š URL çš„ç½‘é¡µå†…å®¹å¹¶æå–çº¯æ–‡æœ¬ã€‚é€‚åˆé˜…è¯»æ–‡ç« ã€æ–‡æ¡£ã€API æ–‡æ¡£ç­‰ã€‚"
            "è‡ªåŠ¨å»é™¤ HTML æ ‡ç­¾ï¼Œæå–ä¸»è¦æ–‡æœ¬å†…å®¹ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "url": {"type": "string", "description": "è¦æŠ“å–çš„ç½‘é¡µ URL"},
                "max_length": {"type": "integer", "description": "è¿”å›å†…å®¹æœ€å¤§å­—ç¬¦æ•°ï¼Œé»˜è®¤ 8000"}
            },
            "required": ["url"]
        }
    }
}


def _html_to_text(html: str) -> str:
    """ç®€æ˜“ HTML è½¬çº¯æ–‡æœ¬"""
    # ç§»é™¤ script å’Œ style
    html = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
    html = re.sub(r'<style[^>]*>.*?</style>', '', html, flags=re.DOTALL | re.IGNORECASE)
    # ç§»é™¤ HTML æ ‡ç­¾
    html = re.sub(r'<[^>]+>', ' ', html)
    # å¤„ç† HTML å®ä½“
    html = html.replace('&nbsp;', ' ').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    html = html.replace('&quot;', '"').replace('&#39;', "'")
    # åˆå¹¶ç©ºç™½
    html = re.sub(r'\s+', ' ', html).strip()
    # æŒ‰æ®µè½åˆ†è¡Œ
    html = re.sub(r' {2,}', '\n', html)
    return html


def _validate_outbound_url(url: str):
    """Basic SSRF guard: allow only public http(s) URLs."""
    try:
        parsed = urllib.parse.urlparse(url)
    except Exception:
        return "âŒ URL è§£æå¤±è´¥"

    if parsed.scheme not in ("http", "https"):
        return "âŒ ä»…æ”¯æŒ http/https URL"

    hostname = (parsed.hostname or "").strip().lower()
    if not hostname:
        return "âŒ URL ç¼ºå°‘ä¸»æœºå"

    blocked_hosts = {"localhost", "127.0.0.1", "::1", "0.0.0.0"}
    if hostname in blocked_hosts:
        return "âŒ å‡ºç«™è¯·æ±‚è¢«å®‰å…¨ç­–ç•¥æ‹¦æˆªï¼šç¦æ­¢è®¿é—®æœ¬æœºåœ°å€"

    try:
        resolved = socket.getaddrinfo(hostname, parsed.port or (443 if parsed.scheme == "https" else 80), proto=socket.IPPROTO_TCP)
    except Exception:
        return "âŒ æ— æ³•è§£æç›®æ ‡ä¸»æœºå"

    for item in resolved:
        ip_str = item[4][0]
        try:
            ip = ipaddress.ip_address(ip_str)
        except ValueError:
            continue

        if (
            ip.is_private
            or ip.is_loopback
            or ip.is_link_local
            or ip.is_reserved
            or ip.is_multicast
        ):
            return f"âŒ å‡ºç«™è¯·æ±‚è¢«å®‰å…¨ç­–ç•¥æ‹¦æˆªï¼šç¦æ­¢è®¿é—®å†…ç½‘/ä¿ç•™åœ°å€ ({ip})"

    return None


@register(fetch_url_schema)
def fetch_url(url: str, max_length: int = 8000):
    """æŠ“å–ç½‘é¡µå†…å®¹"""
    try:
        import urllib.request
        import urllib.error

        url_err = _validate_outbound_url(url)
        if url_err:
            return url_err

        max_length = min(int(max_length) if max_length else 8000, 30000)

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=15) as response:
            content_type = response.headers.get('Content-Type', '')
            charset = 'utf-8'
            if 'charset=' in content_type:
                charset = content_type.split('charset=')[-1].strip()

            raw = response.read()
            try:
                html = raw.decode(charset)
            except (UnicodeDecodeError, LookupError):
                html = raw.decode('utf-8', errors='ignore')

        # æå–æ ‡é¢˜
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html, re.IGNORECASE | re.DOTALL)
        title = title_match.group(1).strip() if title_match else "(æ— æ ‡é¢˜)"

        text = _html_to_text(html)

        if len(text) > max_length:
            text = text[:max_length] + f"\n\n... (å†…å®¹å·²æˆªæ–­ï¼Œå…± {len(text)} å­—ç¬¦)"

        return f"ğŸŒ [{title}]\nğŸ“ {url}\n\n{text}"

    except urllib.error.HTTPError as e:
        return f"âŒ HTTP é”™è¯¯ {e.code}: {e.reason}"
    except urllib.error.URLError as e:
        return f"âŒ ç½‘ç»œé”™è¯¯: {e.reason}"
    except Exception as e:
        return f"âŒ æŠ“å–å¤±è´¥: {e}"


# ==========================================
# 2. ç½‘ç»œæœç´¢ (å¤šå¼•æ“æ¥å£)
# ==========================================
web_search_schema = {
    "type": "function",
    "function": {
        "name": "web_search",
        "description": (
            "é€šè¿‡æœç´¢å¼•æ“æœç´¢ä¿¡æ¯ã€‚æ”¯æŒå¤šä¸ªæœç´¢ API: serper (Google), bing, duckduckgoã€‚"
            "éœ€è¦åœ¨ .env ä¸­é…ç½®å¯¹åº”çš„ API Key (SERPER_API_KEY / BING_API_KEY)ã€‚"
            "å¦‚æœæ²¡æœ‰é…ç½®ä»»ä½•æœç´¢ APIï¼Œå°†ä½¿ç”¨ DuckDuckGo çš„å…è´¹ HTML æœç´¢ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "query": {"type": "string", "description": "æœç´¢å…³é”®è¯"},
                "engine": {"type": "string", "description": "æœç´¢å¼•æ“ (serper/bing/duckduckgo)ï¼Œé»˜è®¤è‡ªåŠ¨é€‰æ‹©"},
                "num_results": {"type": "integer", "description": "è¿”å›ç»“æœæ•°ï¼Œé»˜è®¤ 5"}
            },
            "required": ["query"]
        }
    }
}


def _get_env_value(key: str) -> str:
    value = (os.getenv(key) or "").strip()
    if value:
        return value

    env_candidates = [
        Path.cwd() / ".env",
        Path(__file__).resolve().parent.parent / ".env",
    ]
    for env_path in env_candidates:
        if not env_path.exists():
            continue
        try:
            with open(env_path, "r", encoding="utf-8") as f:
                for line in f:
                    text = line.strip()
                    if not text or text.startswith("#") or "=" not in text:
                        continue
                    k, v = text.split("=", 1)
                    if k.strip() == key:
                        return v.strip().strip('"').strip("'")
        except Exception:
            continue
    return ""


def _search_key_status() -> dict[str, bool]:
    return {
        "SERPER_API_KEY": bool(_get_env_value("SERPER_API_KEY")),
        "BING_API_KEY": bool(_get_env_value("BING_API_KEY")),
    }


def _short_error(err: Exception) -> str:
    raw = str(err).replace("\n", " ").strip() or err.__class__.__name__
    if len(raw) > 180:
        return raw[:177] + "..."
    return raw


def _search_serper(query: str, num: int):
    """ä½¿ç”¨ Serper API (Google Search)"""
    import urllib.request
    api_key = _get_env_value("SERPER_API_KEY")
    if not api_key:
        return None

    data = json.dumps({"q": query, "num": num}).encode()
    req = urllib.request.Request(
        "https://google.serper.dev/search",
        data=data,
        headers={"X-API-KEY": api_key, "Content-Type": "application/json"}
    )
    with urllib.request.urlopen(req, timeout=10) as resp:
        result = json.loads(resp.read().decode())

    items = []
    for r in result.get("organic", [])[:num]:
        items.append({
            "title": r.get("title", ""),
            "url": r.get("link", ""),
            "snippet": r.get("snippet", "")
        })
    return items


def _search_bing(query: str, num: int):
    """ä½¿ç”¨ Bing Search API"""
    import urllib.request
    api_key = _get_env_value("BING_API_KEY")
    if not api_key:
        return None

    url = f"https://api.bing.microsoft.com/v7.0/search?q={urllib.parse.quote(query)}&count={num}"
    req = urllib.request.Request(url, headers={"Ocp-Apim-Subscription-Key": api_key})
    with urllib.request.urlopen(req, timeout=10) as resp:
        result = json.loads(resp.read().decode())

    items = []
    for r in result.get("webPages", {}).get("value", [])[:num]:
        items.append({
            "title": r.get("name", ""),
            "url": r.get("url", ""),
            "snippet": r.get("snippet", "")
        })
    return items


def _search_duckduckgo(query: str, num: int):
    """ä½¿ç”¨ DuckDuckGo HTML æœç´¢ (å…è´¹ï¼Œæ— éœ€ Key)"""
    import urllib.request
    import urllib.parse

    url = f"https://html.duckduckgo.com/html/?q={urllib.parse.quote(query)}"
    req = urllib.request.Request(url, headers={
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    })
    with urllib.request.urlopen(req, timeout=10) as resp:
        html = resp.read().decode('utf-8', errors='ignore')

    items = []
    # ç®€æ˜“è§£æ DuckDuckGo HTML ç»“æœ
    title_hits = re.findall(
        r'<a[^>]*class="result__a"[^>]*href="(.*?)"[^>]*>(.*?)</a>',
        html,
        re.DOTALL | re.IGNORECASE,
    )
    snippet_hits = re.findall(
        r'<(?:a|div|span)[^>]*class="result__snippet"[^>]*>(.*?)</(?:a|div|span)>',
        html,
        re.DOTALL | re.IGNORECASE,
    )
    for idx, (href, title) in enumerate(title_hits[:num]):
        parsed_href = href.strip()
        if parsed_href.startswith("/l/?"):
            params = urllib.parse.parse_qs(urllib.parse.urlparse(parsed_href).query)
            parsed_href = params.get("uddg", [parsed_href])[0]
        parsed_href = html_lib.unescape(parsed_href)
        title = html_lib.unescape(re.sub(r"<[^>]+>", "", title)).strip()
        snippet_raw = snippet_hits[idx] if idx < len(snippet_hits) else ""
        snippet = html_lib.unescape(re.sub(r"<[^>]+>", "", snippet_raw)).strip()
        items.append({"title": title, "url": parsed_href, "snippet": snippet})

    return items


@register(web_search_schema)
def web_search(query: str, engine: str = "", num_results: int = 5):
    """ç½‘ç»œæœç´¢"""
    try:
        num_results = min(int(num_results) if num_results else 5, 15)

        items = None
        used_engine = ""
        attempt_errors: list[str] = []
        missing_key_engines: list[str] = []

        engine_map = {
            "serper": (_search_serper, "Serper (Google)"),
            "bing": (_search_bing, "Bing"),
            "duckduckgo": (_search_duckduckgo, "DuckDuckGo"),
        }

        engine = engine.lower().strip()
        if engine and engine not in engine_map:
            return "âŒ æœç´¢å¤±è´¥: engine ä»…æ”¯æŒ serper / bing / duckduckgo"

        ordered_engines = [engine_map[engine]] if engine else [
            engine_map["serper"],
            engine_map["bing"],
            engine_map["duckduckgo"],
        ]

        for try_engine, try_name in ordered_engines:
            try:
                result = try_engine(query, num_results)
            except Exception as e:
                attempt_errors.append(f"{try_name}: {_short_error(e)}")
                continue
            if result is None:
                missing_key_engines.append(try_name)
                continue
            items = result
            used_engine = try_name
            break

        if items is None:
            key_status = _search_key_status()
            has_search_key = key_status["SERPER_API_KEY"] or key_status["BING_API_KEY"]
            lines = []
            if has_search_key:
                lines.append("âŒ æœç´¢å¤±è´¥: å·²æ£€æµ‹åˆ°æœç´¢ Keyï¼Œä½†è¯·æ±‚æœªæˆåŠŸã€‚")
                lines.append("è¯·æ£€æŸ¥ API Key æ˜¯å¦æœ‰æ•ˆï¼Œä»¥åŠç½‘ç»œæ˜¯å¦å¯è®¿é—®æœç´¢æœåŠ¡ã€‚")
            else:
                lines.append("âŒ æœç´¢å¤±è´¥: å½“å‰æœªæ£€æµ‹åˆ°æœç´¢ API Keyã€‚")
                lines.append("è¯·åœ¨é¡¹ç›®æ ¹ç›®å½• `.env` é‡Œé…ç½®è‡³å°‘ä¸€ä¸ªï¼š")
                lines.append("  SERPER_API_KEY=ä½ çš„_serper_key")
                lines.append("  BING_API_KEY=ä½ çš„_bing_key")
                lines.append("ä¿å­˜åé‡å¯è¿›ç¨‹ï¼ˆ`python run.py` æˆ– `python run_api.py`ï¼‰ã€‚")
            if missing_key_engines:
                lines.append("æœªé…ç½®å¼•æ“: " + ", ".join(missing_key_engines))
            if attempt_errors:
                lines.append("æœ€è¿‘é”™è¯¯: " + " | ".join(attempt_errors[:2]))
            return "\n".join(lines)

        if not items:
            return f"ğŸ” æœç´¢ '{query}' æ— ç»“æœ ({used_engine})"

        lines = [f"ğŸ” æœç´¢: '{query}' ({used_engine}, {len(items)} æ¡ç»“æœ)\n"]
        for i, item in enumerate(items):
            lines.append(f"  {i+1}. **{item['title']}**")
            lines.append(f"     ğŸ”— {item['url']}")
            if item['snippet']:
                lines.append(f"     {item['snippet']}")
            lines.append("")

        return "\n".join(lines)

    except Exception as e:
        return f"âŒ æœç´¢å¤±è´¥: {e}"
