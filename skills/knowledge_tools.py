# skills/knowledge_tools.py
# çŸ¥è¯†åº“å·¥å…·ï¼šä»æ–‡ä»¶/ç›®å½•/ç½‘é¡µæ„å»ºç»“æ„åŒ–çŸ¥è¯†åº“ï¼Œæ”¯æŒè¯­ä¹‰æ£€ç´¢

import os
import json
import time
import hashlib
import re
from .registry import register
from .path_safety import guard_path

KB_DIR = "data/knowledge_base"


def _normalize_kb_name(kb_name: str):
    name = (kb_name or "").strip()
    if not name:
        return None, "âŒ çŸ¥è¯†åº“åç§°ä¸èƒ½ä¸ºç©º"
    if not re.fullmatch(r"[A-Za-z0-9_-]{1,64}", name):
        return None, "âŒ çŸ¥è¯†åº“åç§°åªå…è®¸å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿å’ŒçŸ­æ¨ªçº¿ï¼Œé•¿åº¦ 1-64"
    return name, None


def _ensure_kb_dir():
    if not os.path.exists(KB_DIR):
        os.makedirs(KB_DIR)


# ==========================================
# 1. æ„å»ºçŸ¥è¯†åº“
# ==========================================
kb_build_schema = {
    "type": "function",
    "function": {
        "name": "kb_build",
        "description": (
            "ä»æ–‡ä»¶æˆ–ç›®å½•æ„å»ºçŸ¥è¯†åº“ã€‚è‡ªåŠ¨è¯»å–æ–‡æœ¬æ–‡ä»¶ã€PDFã€Markdownï¼Œ"
            "å°†å†…å®¹åˆ†å—åå­˜å…¥å‘é‡æ•°æ®åº“ (chromadb) ä»¥æ”¯æŒè¯­ä¹‰æœç´¢ã€‚"
            "æ¯ä¸ªçŸ¥è¯†åº“æœ‰ç‹¬ç«‹çš„åç§°å’Œå­˜å‚¨ç©ºé—´ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "kb_name": {"type": "string", "description": "çŸ¥è¯†åº“åç§° (è‹±æ–‡ï¼Œå¦‚ 'python_docs')"},
                "source_path": {"type": "string", "description": "æºæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„"},
                "file_pattern": {"type": "string", "description": "æ–‡ä»¶è¿‡æ»¤ (glob æ¨¡å¼ï¼Œå¦‚ '*.md')ï¼Œé»˜è®¤æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶"},
                "chunk_size": {"type": "integer", "description": "åˆ†å—å¤§å° (å­—ç¬¦æ•°)ï¼Œé»˜è®¤ 500"}
            },
            "required": ["kb_name", "source_path"]
        }
    }
}


TEXT_EXTS = {'.txt', '.md', '.py', '.js', '.ts', '.json', '.yaml', '.yml',
             '.html', '.css', '.csv', '.xml', '.rst', '.ini', '.cfg', '.toml',
             '.java', '.c', '.cpp', '.h', '.go', '.rs', '.rb', '.php', '.sql'}


def _chunk_text(text: str, chunk_size: int = 500) -> list:
    """å°†æ–‡æœ¬åˆ†å—"""
    chunks = []
    paragraphs = text.split('\n\n')
    current = ""

    for para in paragraphs:
        if len(current) + len(para) + 2 > chunk_size and current:
            chunks.append(current.strip())
            current = para
        else:
            current += ("\n\n" if current else "") + para

    if current.strip():
        chunks.append(current.strip())

    # å¯¹è¶…å¤§å—å†æ¬¡åˆ†å‰²
    final = []
    for chunk in chunks:
        while len(chunk) > chunk_size * 2:
            split_pos = chunk.rfind('\n', 0, chunk_size)
            if split_pos == -1:
                split_pos = chunk_size
            final.append(chunk[:split_pos].strip())
            chunk = chunk[split_pos:].strip()
        if chunk:
            final.append(chunk)

    return final


@register(kb_build_schema)
def kb_build(kb_name: str, source_path: str, file_pattern: str = "", chunk_size: int = 500):
    """æ„å»ºçŸ¥è¯†åº“"""
    try:
        import chromadb
        import fnmatch

        _ensure_kb_dir()
        chunk_size = max(200, min(int(chunk_size) if chunk_size else 500, 2000))

        kb_name, kb_name_err = _normalize_kb_name(kb_name)
        if kb_name_err:
            return kb_name_err
        kb_path = os.path.join(KB_DIR, kb_name)

        source_obj, err = guard_path(source_path, must_exist=True, for_write=False)
        if err:
            return err

        client = chromadb.PersistentClient(path=kb_path)
        collection = client.get_or_create_collection(
            name="documents",
            metadata={"hnsw:space": "cosine"}
        )

        # æ”¶é›†è¦å¤„ç†çš„æ–‡ä»¶
        files = []
        if source_obj.is_file():
            files = [str(source_obj)]
        elif source_obj.is_dir():
            for root, dirs, fnames in os.walk(source_obj):
                dirs[:] = [d for d in dirs if d not in {'.git', '__pycache__', 'venv', 'node_modules'}]
                for fname in fnames:
                    if file_pattern and not fnmatch.fnmatch(fname, file_pattern):
                        continue
                    ext = os.path.splitext(fname)[1].lower()
                    if not file_pattern and ext not in TEXT_EXTS:
                        continue
                    files.append(os.path.join(root, fname))
        else:
            return f"âŒ è·¯å¾„ä¸å­˜åœ¨: {source_obj}"

        if not files:
            return f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…çš„æ–‡ä»¶"

        total_chunks = 0
        processed_files = 0
        replaced_chunks = 0
        failed_files = 0

        for filepath in files:
            try:
                source_abs = os.path.abspath(filepath)

                # PDF ç‰¹æ®Šå¤„ç†
                if source_abs.lower().endswith('.pdf'):
                    try:
                        import PyPDF2
                        with open(source_abs, 'rb') as f:
                            reader = PyPDF2.PdfReader(f)
                            text = "\n\n".join(page.extract_text() or "" for page in reader.pages)
                    except ImportError:
                        continue
                else:
                    with open(source_abs, 'r', encoding='utf-8', errors='ignore') as f:
                        text = f.read()

                if not text.strip():
                    continue

                chunks = _chunk_text(text, chunk_size)

                ids = []
                documents = []
                metadatas = []

                source_id = hashlib.sha1(source_abs.encode('utf-8')).hexdigest()[:16]

                # å…ˆåˆ é™¤è¯¥æ¥æºæ—§æ•°æ®ï¼Œé¿å…é‡å¤/å†²çªå †ç§¯
                old = collection.get(where={"source": source_abs}, include=[])
                old_ids = old.get("ids", []) if isinstance(old, dict) else []
                if old_ids:
                    collection.delete(ids=old_ids)
                    replaced_chunks += len(old_ids)

                for i, chunk in enumerate(chunks):
                    chunk_hash = hashlib.sha1(chunk.encode('utf-8')).hexdigest()[:12]
                    doc_id = f"{source_id}_{i}_{chunk_hash}"
                    ids.append(doc_id)
                    documents.append(chunk)
                    metadatas.append({
                        "source": source_abs,
                        "chunk_index": str(i),
                        "total_chunks": str(len(chunks))
                    })

                if documents:
                    collection.add(ids=ids, documents=documents, metadatas=metadatas)
                    total_chunks += len(documents)
                    processed_files += 1

            except Exception as e:
                failed_files += 1
                continue  # è·³è¿‡è¯»å–å¤±è´¥çš„æ–‡ä»¶

        return (
            f"âœ… çŸ¥è¯†åº“ '{kb_name}' æ„å»ºå®Œæˆ:\n"
            f"  å¤„ç†æ–‡ä»¶: {processed_files}/{len(files)}\n"
            f"  æ›¿æ¢æ—§å—: {replaced_chunks}\n"
            f"  æ–‡æ¡£åˆ†å—: {total_chunks}\n"
            f"  å¤±è´¥æ–‡ä»¶: {failed_files}\n"
            f"  å­˜å‚¨ä½ç½®: {kb_path}"
        )

    except ImportError:
        return "âŒ éœ€è¦å®‰è£… chromadb: pip install chromadb"
    except Exception as e:
        return f"âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥: {e}"


# ==========================================
# 2. æŸ¥è¯¢çŸ¥è¯†åº“
# ==========================================
kb_query_schema = {
    "type": "function",
    "function": {
        "name": "kb_query",
        "description": (
            "åœ¨çŸ¥è¯†åº“ä¸­è¿›è¡Œè¯­ä¹‰æœç´¢ã€‚è¾“å…¥è‡ªç„¶è¯­è¨€æŸ¥è¯¢ï¼Œè¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µã€‚"
            "é€‚åˆåœ¨å·²æ„å»ºçš„çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ç‰¹å®šä¿¡æ¯ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "kb_name": {"type": "string", "description": "çŸ¥è¯†åº“åç§°"},
                "query": {"type": "string", "description": "æœç´¢æŸ¥è¯¢ (è‡ªç„¶è¯­è¨€)"},
                "top_k": {"type": "integer", "description": "è¿”å›æœ€ç›¸å…³çš„æ¡æ•°ï¼Œé»˜è®¤ 5"}
            },
            "required": ["kb_name", "query"]
        }
    }
}


@register(kb_query_schema)
def kb_query(kb_name: str, query: str, top_k: int = 5):
    """æŸ¥è¯¢çŸ¥è¯†åº“"""
    try:
        import chromadb

        kb_name, kb_name_err = _normalize_kb_name(kb_name)
        if kb_name_err:
            return kb_name_err

        kb_path = os.path.join(KB_DIR, kb_name)
        if not os.path.exists(kb_path):
            return f"âŒ çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_name}"

        top_k = max(1, min(int(top_k) if top_k else 5, 20))

        client = chromadb.PersistentClient(path=kb_path)
        collection = client.get_collection("documents")

        count = collection.count()
        actual_k = min(top_k, count)
        if actual_k == 0:
            return f"ğŸ“­ çŸ¥è¯†åº“ '{kb_name}' ä¸ºç©º"

        results = collection.query(query_texts=[query], n_results=actual_k)

        if not results['documents'] or not results['documents'][0]:
            return f"ğŸ” æœªæ‰¾åˆ°ç›¸å…³å†…å®¹"

        lines = [f"ğŸ” çŸ¥è¯†åº“ '{kb_name}' æœç´¢ç»“æœ (å…± {count} ä¸ªæ–‡æ¡£å—):\n"]

        for i, (doc, meta, dist) in enumerate(zip(
            results['documents'][0],
            results['metadatas'][0],
            results['distances'][0]
        )):
            similarity = max(0, 1 - dist)
            source = meta.get('source', 'æœªçŸ¥')
            chunk_idx = meta.get('chunk_index', '?')
            preview = doc[:300] + "..." if len(doc) > 300 else doc
            lines.append(f"  [{i+1}] ({similarity:.0%}) æ¥æº: {source} (å—#{chunk_idx})")
            lines.append(f"      {preview}\n")

        return "\n".join(lines)

    except Exception as e:
        return f"âŒ çŸ¥è¯†åº“æŸ¥è¯¢å¤±è´¥: {e}"


# ==========================================
# 3. ç®¡ç†çŸ¥è¯†åº“
# ==========================================
kb_manage_schema = {
    "type": "function",
    "function": {
        "name": "kb_manage",
        "description": "ç®¡ç†çŸ¥è¯†åº“ï¼šåˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“ã€æŸ¥çœ‹çŠ¶æ€ã€åˆ é™¤çŸ¥è¯†åº“ã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "action": {"type": "string", "description": "æ“ä½œ: list(åˆ—å‡º), status(æŸ¥çœ‹çŠ¶æ€), delete(åˆ é™¤)"},
                "kb_name": {"type": "string", "description": "çŸ¥è¯†åº“åç§° (status/delete æ—¶éœ€è¦)"}
            },
            "required": ["action"]
        }
    }
}


@register(kb_manage_schema)
def kb_manage(action: str, kb_name: str = ""):
    try:
        _ensure_kb_dir()
        import chromadb

        if action == "list":
            entries = [d for d in os.listdir(KB_DIR)
                      if os.path.isdir(os.path.join(KB_DIR, d))]
            if not entries:
                return "ğŸ“š æš‚æ— çŸ¥è¯†åº“"

            lines = ["ğŸ“š çŸ¥è¯†åº“åˆ—è¡¨:\n"]
            for name in sorted(entries):
                try:
                    client = chromadb.PersistentClient(path=os.path.join(KB_DIR, name))
                    col = client.get_collection("documents")
                    count = col.count()
                    lines.append(f"  ğŸ“ {name} ({count} ä¸ªæ–‡æ¡£å—)")
                except Exception:
                    lines.append(f"  ğŸ“ {name} (æ— æ³•è¯»å–)")
            return "\n".join(lines)

        elif action == "status":
            kb_name, kb_name_err = _normalize_kb_name(kb_name)
            if kb_name_err:
                return kb_name_err
            kb_path = os.path.join(KB_DIR, kb_name)
            if not os.path.exists(kb_path):
                return f"âŒ çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_name}"

            client = chromadb.PersistentClient(path=kb_path)
            col = client.get_collection("documents")
            count = col.count()

            # ç»Ÿè®¡æ¥æºæ–‡ä»¶
            if count > 0:
                sample = col.get(limit=min(count, 100), include=["metadatas"])
                sources = set()
                for m in sample['metadatas']:
                    sources.add(m.get('source', 'æœªçŸ¥'))
                return (
                    f"ğŸ“Š çŸ¥è¯†åº“ '{kb_name}':\n"
                    f"  æ–‡æ¡£å—æ•°: {count}\n"
                    f"  æ¥æºæ–‡ä»¶: {len(sources)}\n"
                    f"  å­˜å‚¨è·¯å¾„: {kb_path}"
                )
            return f"ğŸ“Š çŸ¥è¯†åº“ '{kb_name}': ç©º (0 ä¸ªæ–‡æ¡£å—)"

        elif action == "delete":
            kb_name, kb_name_err = _normalize_kb_name(kb_name)
            if kb_name_err:
                return kb_name_err
            kb_path = os.path.join(KB_DIR, kb_name)
            if not os.path.exists(kb_path):
                return f"âŒ çŸ¥è¯†åº“ä¸å­˜åœ¨: {kb_name}"
            import shutil
            shutil.rmtree(kb_path)
            return f"âœ… å·²åˆ é™¤çŸ¥è¯†åº“: {kb_name}"

        else:
            return f"âŒ æœªçŸ¥æ“ä½œ: {action}ã€‚æ”¯æŒ: list, status, delete"

    except Exception as e:
        return f"âŒ çŸ¥è¯†åº“ç®¡ç†å¤±è´¥: {e}"
