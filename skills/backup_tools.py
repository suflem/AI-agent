# skills/backup_tools.py
# ç¼–è¾‘å¤‡ä»½ä¸æ’¤é”€ç³»ç»Ÿï¼šæ¯æ¬¡æ–‡ä»¶ç¼–è¾‘å‰è‡ªåŠ¨å¤‡ä»½ï¼Œæ”¯æŒæŸ¥çœ‹å†å²å’Œå›æ»š

import os
import json
import time
import shutil
from pathlib import Path
from .registry import register
from .path_safety import guard_path, WORKSPACE_ROOT

BACKUP_DIR = "data/backups"
BACKUP_INDEX = "data/backups/_index.json"
MAX_BACKUPS_PER_FILE = 10


def _display_path(path_obj):
    try:
        return str(path_obj.relative_to(WORKSPACE_ROOT))
    except Exception:
        return str(path_obj)


def _ensure_backup_dir():
    dir_obj, err = guard_path(BACKUP_DIR, must_exist=False, for_write=True)
    if err:
        raise ValueError(err)
    if not dir_obj.exists():
        dir_obj.mkdir(parents=True, exist_ok=True)
    return dir_obj


def _load_index():
    idx_obj, err = guard_path(BACKUP_INDEX, must_exist=False, for_write=False)
    if err:
        return {}
    if idx_obj and idx_obj.exists():
        try:
            with open(idx_obj, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            pass
    return {}


def _save_index(index):
    idx_obj, err = guard_path(BACKUP_INDEX, must_exist=False, for_write=True)
    if err:
        raise ValueError(err)
    if not idx_obj.parent.exists():
        idx_obj.parent.mkdir(parents=True, exist_ok=True)
    with open(idx_obj, 'w', encoding='utf-8') as f:
        json.dump(index, f, ensure_ascii=False, indent=2)


def _file_key(file_path: Path) -> str:
    """Generate a stable key for indexing backups by file."""
    try:
        return file_path.resolve().relative_to(WORKSPACE_ROOT).as_posix()
    except Exception:
        return str(file_path.resolve())


def create_backup(file_path) -> str:
    """Create a backup of file_path before editing. Returns backup path or empty string on failure.
    Called by edit_tools and other write tools before modifying files.
    """
    try:
        if isinstance(file_path, str):
            file_path = Path(file_path)

        if not file_path.exists() or not file_path.is_file():
            return ""

        backup_dir = _ensure_backup_dir()
        key = _file_key(file_path)
        timestamp = time.strftime("%Y%m%d_%H%M%S")

        # Build safe backup filename: flatten path separators
        safe_name = key.replace("/", "__").replace("\\", "__")
        backup_name = f"{safe_name}.{timestamp}.bak"

        backup_obj, err = guard_path(str(backup_dir / backup_name), must_exist=False, for_write=True)
        if err:
            return ""

        shutil.copy2(str(file_path), str(backup_obj))

        # Update index
        index = _load_index()
        entries = index.setdefault(key, [])
        entries.append({
            "backup_file": str(backup_obj.name),
            "timestamp": timestamp,
            "size": file_path.stat().st_size,
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        })

        # Prune old backups
        if len(entries) > MAX_BACKUPS_PER_FILE:
            removed = entries[:-MAX_BACKUPS_PER_FILE]
            entries[:] = entries[-MAX_BACKUPS_PER_FILE:]
            for old in removed:
                old_path = backup_dir / old["backup_file"]
                if old_path.exists():
                    old_path.unlink()

        index[key] = entries
        _save_index(index)
        return str(backup_obj)

    except Exception:
        return ""


# ==========================================
# 1. æŸ¥çœ‹å¤‡ä»½å†å²
# ==========================================
backup_history_schema = {
    "type": "function",
    "function": {
        "name": "backup_history",
        "description": (
            "æŸ¥çœ‹æ–‡ä»¶çš„ç¼–è¾‘å¤‡ä»½å†å²ã€‚æ¯æ¬¡é€šè¿‡ edit_file / multi_edit ç­‰å·¥å…·"
            "ä¿®æ”¹æ–‡ä»¶æ—¶éƒ½ä¼šè‡ªåŠ¨åˆ›å»ºå¤‡ä»½ï¼Œæ­¤å·¥å…·å¯ä»¥åˆ—å‡ºæŒ‡å®šæ–‡ä»¶çš„æ‰€æœ‰å¤‡ä»½ç‰ˆæœ¬ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "filepath": {"type": "string", "description": "è¦æŸ¥çœ‹å¤‡ä»½å†å²çš„æ–‡ä»¶è·¯å¾„"},
            },
            "required": ["filepath"],
        },
    },
}


@register(backup_history_schema)
def backup_history(filepath: str):
    """æŸ¥çœ‹æ–‡ä»¶çš„å¤‡ä»½å†å²"""
    try:
        file_obj, err = guard_path(filepath, must_exist=False, for_write=False)
        if err:
            return err

        key = _file_key(file_obj)
        index = _load_index()
        entries = index.get(key, [])

        if not entries:
            return f"ğŸ“‚ æ–‡ä»¶ '{_display_path(file_obj)}' æš‚æ— å¤‡ä»½è®°å½•"

        lines = [f"ğŸ“‚ æ–‡ä»¶ '{_display_path(file_obj)}' çš„å¤‡ä»½å†å² ({len(entries)} ä¸ªç‰ˆæœ¬):\n"]
        for i, entry in enumerate(reversed(entries)):
            age_label = "æœ€æ–°" if i == 0 else f"#{i+1}"
            size_kb = entry.get("size", 0) / 1024
            lines.append(
                f"  [{age_label}] {entry['created_at']}  "
                f"({size_kb:.1f} KB)  {entry['backup_file']}"
            )

        lines.append(f"\nğŸ’¡ ä½¿ç”¨ undo_edit å¯ä»¥æ¢å¤åˆ°ä»»æ„ç‰ˆæœ¬ã€‚")
        return "\n".join(lines)

    except Exception as e:
        return f"âŒ æŸ¥çœ‹å¤‡ä»½å†å²å¤±è´¥: {e}"


# ==========================================
# 2. æ’¤é”€ç¼–è¾‘ (æ¢å¤å¤‡ä»½)
# ==========================================
undo_edit_schema = {
    "type": "function",
    "function": {
        "name": "undo_edit",
        "description": (
            "ã€å±é™©æ“ä½œã€‘æ’¤é”€æ–‡ä»¶ç¼–è¾‘ï¼Œæ¢å¤åˆ°ä¹‹å‰çš„å¤‡ä»½ç‰ˆæœ¬ã€‚"
            "é»˜è®¤æ¢å¤åˆ°æœ€è¿‘ä¸€æ¬¡å¤‡ä»½ã€‚å¯é€šè¿‡ version æŒ‡å®šæ¢å¤åˆ°ç¬¬å‡ ä¸ªå†å²ç‰ˆæœ¬ (1=æœ€æ–°å¤‡ä»½)ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "filepath": {"type": "string", "description": "è¦æ¢å¤çš„æ–‡ä»¶è·¯å¾„"},
                "version": {
                    "type": "integer",
                    "description": "æ¢å¤åˆ°ç¬¬å‡ ä¸ªç‰ˆæœ¬ (1=æœ€æ–°å¤‡ä»½, 2=å€’æ•°ç¬¬äºŒä¸ª, ...)ï¼Œé»˜è®¤ 1",
                },
            },
            "required": ["filepath"],
        },
    },
}


@register(undo_edit_schema)
def undo_edit(filepath: str, version: int = 1):
    """æ’¤é”€ç¼–è¾‘ï¼Œæ¢å¤å¤‡ä»½"""
    try:
        file_obj, err = guard_path(filepath, must_exist=False, for_write=True)
        if err:
            return err

        key = _file_key(file_obj)
        index = _load_index()
        entries = index.get(key, [])

        if not entries:
            return f"âŒ æ–‡ä»¶ '{_display_path(file_obj)}' æ²¡æœ‰å¯ç”¨çš„å¤‡ä»½"

        version = max(1, min(int(version) if version else 1, len(entries)))
        target = entries[-version]

        backup_dir = _ensure_backup_dir()
        backup_file = backup_dir / target["backup_file"]

        if not backup_file.exists():
            return f"âŒ å¤‡ä»½æ–‡ä»¶ç¼ºå¤±: {target['backup_file']}"

        # Before restoring, create a backup of the current version (safety net)
        if file_obj.exists():
            create_backup(file_obj)

        shutil.copy2(str(backup_file), str(file_obj))

        return (
            f"âœ… å·²æ¢å¤ '{_display_path(file_obj)}' åˆ°ç‰ˆæœ¬ {target['created_at']}\n"
            f"  å¤‡ä»½æ¥æº: {target['backup_file']}\n"
            f"  åŸå§‹å¤§å°: {target.get('size', '?')} å­—èŠ‚\n"
            f"  ğŸ’¡ æ¢å¤å‰çš„ç‰ˆæœ¬ä¹Ÿå·²è‡ªåŠ¨å¤‡ä»½ã€‚"
        )

    except Exception as e:
        return f"âŒ æ’¤é”€å¤±è´¥: {e}"


# ==========================================
# 3. æ¸…ç†å¤‡ä»½
# ==========================================
backup_clean_schema = {
    "type": "function",
    "function": {
        "name": "backup_clean",
        "description": "æ¸…ç†æ–‡ä»¶å¤‡ä»½ã€‚å¯ä»¥æ¸…ç†æŒ‡å®šæ–‡ä»¶çš„å¤‡ä»½ï¼Œæˆ–æ¸…ç†æ‰€æœ‰å¤‡ä»½ã€‚",
        "parameters": {
            "type": "object",
            "properties": {
                "filepath": {"type": "string", "description": "è¦æ¸…ç†çš„æ–‡ä»¶è·¯å¾„ï¼Œç•™ç©ºåˆ™æ¸…ç†å…¨éƒ¨"},
                "keep": {"type": "integer", "description": "æ¯ä¸ªæ–‡ä»¶ä¿ç•™æœ€è¿‘å‡ ä¸ªå¤‡ä»½ï¼Œé»˜è®¤ 3"},
            },
            "required": [],
        },
    },
}


@register(backup_clean_schema)
def backup_clean(filepath: str = "", keep: int = 3):
    """æ¸…ç†å¤‡ä»½"""
    try:
        keep = max(0, min(int(keep) if keep else 3, MAX_BACKUPS_PER_FILE))
        backup_dir = _ensure_backup_dir()
        index = _load_index()

        if filepath:
            file_obj, err = guard_path(filepath, must_exist=False, for_write=False)
            if err:
                return err
            key = _file_key(file_obj)
            keys_to_clean = [key] if key in index else []
        else:
            keys_to_clean = list(index.keys())

        if not keys_to_clean:
            return "ğŸ“‚ æ²¡æœ‰å¯æ¸…ç†çš„å¤‡ä»½"

        total_removed = 0
        for key in keys_to_clean:
            entries = index.get(key, [])
            if len(entries) <= keep:
                continue
            to_remove = entries[:-keep] if keep > 0 else entries
            for old in to_remove:
                old_path = backup_dir / old["backup_file"]
                if old_path.exists():
                    old_path.unlink()
                    total_removed += 1
            index[key] = entries[-keep:] if keep > 0 else []

        # Remove empty keys
        index = {k: v for k, v in index.items() if v}
        _save_index(index)

        scope = f"æ–‡ä»¶ '{filepath}'" if filepath else "å…¨éƒ¨æ–‡ä»¶"
        return f"âœ… å·²æ¸…ç† {scope} çš„å¤‡ä»½ï¼šåˆ é™¤ {total_removed} ä¸ªæ—§ç‰ˆæœ¬ï¼Œæ¯æ–‡ä»¶ä¿ç•™æœ€è¿‘ {keep} ä¸ª"

    except Exception as e:
        return f"âŒ æ¸…ç†å¤‡ä»½å¤±è´¥: {e}"
