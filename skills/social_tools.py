# skills/social_tools.py
# ç¤¾äº¤ä¿¡æ¯æµå¤„ç†æ¨¡å— (OpenClaw-like)
# æ”¯æŒ RSS è®¢é˜…ã€å†…å®¹èšåˆã€ä¿¡æ¯æµæ‘˜è¦ã€ç¤¾äº¤å¹³å° API æ¥å£

import os
import json
import re
import time
import socket
import ipaddress
import urllib.parse
from datetime import datetime
from email.utils import parsedate_to_datetime
from .registry import register
from .path_safety import guard_path

SOCIAL_DATA_DIR = "data/social"


def _ensure_social_dir():
    dir_obj, err = guard_path(SOCIAL_DATA_DIR, must_exist=False, for_write=True)
    if err:
        raise ValueError(err)
    if not dir_obj.exists():
        dir_obj.mkdir(parents=True, exist_ok=True)
    return dir_obj


def _guarded_social_file(filename):
    dir_obj = _ensure_social_dir()
    file_obj, err = guard_path(str(dir_obj / filename), must_exist=False, for_write=True)
    if err:
        raise ValueError(err)
    return file_obj


def _load_connector_meta():
    meta_obj = _guarded_social_file("social_config.json")
    if meta_obj.exists():
        with open(meta_obj, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # å…¼å®¹æ—§æ ¼å¼ï¼šç§»é™¤ä»»ä½•æ˜æ–‡ api_key å­—æ®µ
        changed = False
        if isinstance(data, dict):
            for key, value in list(data.items()):
                if isinstance(value, dict) and "api_key" in value:
                    value.pop("api_key", None)
                    changed = True
                elif not isinstance(value, dict):
                    data[key] = {"configured_at": "", "api_key_env": ""}
                    changed = True

            if changed:
                _save_connector_meta(data)
            return data

        return {}
    return {}


def _save_connector_meta(meta):
    meta_obj = _guarded_social_file("social_config.json")
    with open(meta_obj, 'w', encoding='utf-8') as f:
        json.dump(meta, f, ensure_ascii=False, indent=2)


def _get_platform_api_key(platform: str, runtime_key: str = ""):
    info = SOCIAL_PLATFORMS.get(platform)
    if not info:
        return ""
    if runtime_key:
        return runtime_key.strip()
    return os.getenv(info["api_key_env"], "").strip()


def _validate_public_http_url(url: str):
    try:
        parsed = urllib.parse.urlparse(url)
    except Exception:
        return "âŒ URL è§£æå¤±è´¥"

    if parsed.scheme not in ("http", "https"):
        return "âŒ ä»…æ”¯æŒ http/https çš„ RSS URL"

    host = (parsed.hostname or "").strip().lower()
    if not host:
        return "âŒ URL ç¼ºå°‘ä¸»æœºå"

    blocked_hosts = {"localhost", "127.0.0.1", "::1", "0.0.0.0"}
    if host in blocked_hosts:
        return "âŒ å®‰å…¨æ‹¦æˆªï¼šç¦æ­¢è®¿é—®æœ¬æœºåœ°å€"

    try:
        resolved = socket.getaddrinfo(host, parsed.port or (443 if parsed.scheme == "https" else 80), proto=socket.IPPROTO_TCP)
    except Exception:
        return "âŒ æ— æ³•è§£æç›®æ ‡ä¸»æœº"

    for item in resolved:
        ip_str = item[4][0]
        try:
            ip = ipaddress.ip_address(ip_str)
        except ValueError:
            continue

        if ip.is_private or ip.is_loopback or ip.is_link_local or ip.is_reserved or ip.is_multicast:
            return f"âŒ å®‰å…¨æ‹¦æˆªï¼šç¦æ­¢è®¿é—®å†…ç½‘/ä¿ç•™åœ°å€ ({ip})"

    return None


# ==========================================
# 1. RSS è®¢é˜…ç®¡ç†ä¸å†…å®¹æŠ“å–
# ==========================================
rss_manage_schema = {
    "type": "function",
    "function": {
        "name": "rss_manage",
        "description": (
            "ç®¡ç† RSS/Atom è®¢é˜…æºã€‚æ”¯æŒæ·»åŠ ã€åˆ é™¤ã€åˆ—å‡ºè®¢é˜…æºï¼Œä»¥åŠæŠ“å–æœ€æ–°å†…å®¹ã€‚"
            "é€‚åˆè·Ÿè¸ªåšå®¢ã€æ–°é—»ã€æŠ€æœ¯ç¤¾åŒºæ›´æ–°ç­‰ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "action": {
                    "type": "string",
                    "description": "æ“ä½œ: add(æ·»åŠ è®¢é˜…), remove(åˆ é™¤), list(åˆ—å‡º), fetch(æŠ“å–å†…å®¹), fetch_all(æŠ“å–å…¨éƒ¨)"
                },
                "url": {"type": "string", "description": "RSS æº URL (add/fetch æ—¶ä½¿ç”¨)"},
                "name": {"type": "string", "description": "è®¢é˜…æºåç§° (add æ—¶ä½¿ç”¨)"},
                "category": {"type": "string", "description": "åˆ†ç±»æ ‡ç­¾ (å¦‚ 'tech', 'news')"},
                "max_items": {"type": "integer", "description": "æœ€å¤§è¿”å›æ¡ç›®æ•°ï¼Œé»˜è®¤ 10"}
            },
            "required": ["action"]
        }
    }
}


def _load_feeds():
    feeds_obj = _guarded_social_file("rss_feeds.json")
    if feeds_obj.exists():
        with open(feeds_obj, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []


def _save_feeds(feeds):
    feeds_obj = _guarded_social_file("rss_feeds.json")
    with open(feeds_obj, 'w', encoding='utf-8') as f:
        json.dump(feeds, f, ensure_ascii=False, indent=2)


def _parse_rss(xml_text: str, max_items: int = 10):
    """ç®€æ˜“ RSS/Atom è§£æå™¨ï¼ˆæ— ä¾èµ–ï¼‰"""
    items = []

    # å°è¯• RSS 2.0 æ ¼å¼
    item_blocks = re.findall(r'<item>(.*?)</item>', xml_text, re.DOTALL)
    if not item_blocks:
        # å°è¯• Atom æ ¼å¼
        item_blocks = re.findall(r'<entry>(.*?)</entry>', xml_text, re.DOTALL)

    for block in item_blocks[:max_items]:
        title = re.search(r'<title[^>]*>(.*?)</title>', block, re.DOTALL)
        link = re.search(r'<link[^>]*(?:href=["\']([^"\']+)["\'])?[^>]*>(.*?)</link>', block, re.DOTALL)
        desc = re.search(r'<description[^>]*>(.*?)</description>', block, re.DOTALL)
        if not desc:
            desc = re.search(r'<summary[^>]*>(.*?)</summary>', block, re.DOTALL)
        pub_date = re.search(r'<pubDate[^>]*>(.*?)</pubDate>', block, re.DOTALL)
        if not pub_date:
            pub_date = re.search(r'<updated[^>]*>(.*?)</updated>', block, re.DOTALL)

        item = {
            "title": _clean_html(title.group(1)) if title else "(æ— æ ‡é¢˜)",
            "link": "",
            "description": "",
            "date": pub_date.group(1).strip() if pub_date else ""
        }

        if link:
            item["link"] = link.group(1) or _clean_html(link.group(2))

        if desc:
            d = _clean_html(desc.group(1))
            item["description"] = d[:200] + "..." if len(d) > 200 else d

        items.append(item)

    return items


def _clean_html(text: str) -> str:
    """æ¸…ç† HTML æ ‡ç­¾å’Œ CDATA"""
    text = re.sub(r'<!\[CDATA\[(.*?)\]\]>', r'\1', text, flags=re.DOTALL)
    text = re.sub(r'<[^>]+>', '', text)
    text = text.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
    text = text.replace('&quot;', '"').replace('&#39;', "'").replace('&nbsp;', ' ')
    return text.strip()


def _fetch_rss(url: str, max_items: int = 10):
    """æŠ“å–å¹¶è§£æ RSS"""
    import urllib.request
    url_err = _validate_public_http_url(url)
    if url_err:
        raise ValueError(url_err)

    req = urllib.request.Request(url, headers={
        'User-Agent': 'Mozilla/5.0 (compatible; AIAssistant/1.0)'
    })
    with urllib.request.urlopen(req, timeout=15) as resp:
        xml_text = resp.read().decode('utf-8', errors='ignore')
    return _parse_rss(xml_text, max_items)


@register(rss_manage_schema)
def rss_manage(action: str, url: str = "", name: str = "",
               category: str = "", max_items: int = 10):
    """RSS è®¢é˜…ç®¡ç†"""
    try:
        feeds = _load_feeds()
        max_items = min(int(max_items) if max_items else 10, 30)

        if action == "add":
            if not url:
                return "âŒ è¯·æä¾› RSS æº URL"
            url_err = _validate_public_http_url(url)
            if url_err:
                return url_err
            # æ£€æŸ¥æ˜¯å¦é‡å¤
            for f in feeds:
                if f["url"] == url:
                    return f"âš ï¸ å·²å­˜åœ¨ç›¸åŒè®¢é˜…: {f['name']}"
            feed_name = name or url
            feeds.append({
                "url": url,
                "name": feed_name,
                "category": category,
                "added": time.strftime("%Y-%m-%d %H:%M")
            })
            _save_feeds(feeds)
            return f"âœ… å·²æ·»åŠ  RSS è®¢é˜…: {feed_name}"

        elif action == "remove":
            if not url and not name:
                return "âŒ è¯·æä¾› URL æˆ–åç§°"
            before = len(feeds)
            feeds = [f for f in feeds if f["url"] != url and f["name"] != name]
            if len(feeds) == before:
                return "âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è®¢é˜…"
            _save_feeds(feeds)
            return "âœ… å·²åˆ é™¤è®¢é˜…"

        elif action == "list":
            if not feeds:
                return "ğŸ“¡ æš‚æ—  RSS è®¢é˜…"
            lines = ["ğŸ“¡ RSS è®¢é˜…åˆ—è¡¨:\n"]
            for i, f in enumerate(feeds):
                cat = f" [{f['category']}]" if f.get('category') else ""
                lines.append(f"  {i+1}. {f['name']}{cat}")
                lines.append(f"     ğŸ”— {f['url']}")
            return "\n".join(lines)

        elif action == "fetch":
            if not url:
                return "âŒ è¯·æä¾› RSS æº URL"
            url_err = _validate_public_http_url(url)
            if url_err:
                return url_err
            items = _fetch_rss(url, max_items)
            if not items:
                return f"âš ï¸ æœªè·å–åˆ°å†…å®¹: {url}"
            lines = [f"ğŸ“° RSS å†…å®¹ ({len(items)} æ¡):\n"]
            for i, item in enumerate(items):
                lines.append(f"  {i+1}. **{item['title']}**")
                if item['date']:
                    lines.append(f"     ğŸ“… {item['date']}")
                if item['link']:
                    lines.append(f"     ğŸ”— {item['link']}")
                if item['description']:
                    lines.append(f"     {item['description']}")
                lines.append("")
            return "\n".join(lines)

        elif action == "fetch_all":
            if not feeds:
                return "ğŸ“¡ æš‚æ— è®¢é˜…æº"
            all_items = []
            errors = []
            for f in feeds:
                try:
                    items = _fetch_rss(f["url"], max_items=5)
                    for item in items:
                        item["source"] = f["name"]
                        item["category"] = f.get("category", "")
                    all_items.extend(items)
                except Exception as e:
                    errors.append(f"{f['name']}: {e}")

            if not all_items:
                err_msg = ("\nå¤±è´¥: " + "; ".join(errors)) if errors else ""
                return f"âš ï¸ æœªè·å–åˆ°ä»»ä½•å†…å®¹{err_msg}"

            lines = [f"ğŸ“° ä¿¡æ¯æµæ±‡æ€» ({len(all_items)} æ¡ï¼Œæ¥è‡ª {len(feeds)} ä¸ªæº):\n"]
            for i, item in enumerate(all_items[:max_items]):
                cat = f" [{item['category']}]" if item.get('category') else ""
                lines.append(f"  {i+1}. [{item['source']}]{cat} **{item['title']}**")
                if item['link']:
                    lines.append(f"     ğŸ”— {item['link']}")
                if item['description']:
                    lines.append(f"     {item['description']}")
                lines.append("")

            if errors:
                lines.append(f"âš ï¸ {len(errors)} ä¸ªæºæŠ“å–å¤±è´¥")
            return "\n".join(lines)

        else:
            return f"âŒ æœªçŸ¥æ“ä½œ: {action}ã€‚æ”¯æŒ: add, remove, list, fetch, fetch_all"

    except Exception as e:
        return f"âŒ RSS ç®¡ç†å¤±è´¥: {e}"


# ==========================================
# 2. ä¿¡æ¯æµ AI æ‘˜è¦
# ==========================================
feed_digest_schema = {
    "type": "function",
    "function": {
        "name": "feed_digest",
        "description": (
            "å¯¹ä¿¡æ¯æµå†…å®¹è¿›è¡Œ AI æ™ºèƒ½æ‘˜è¦ã€‚å…ˆæŠ“å–æ‰€æœ‰ RSS è®¢é˜…çš„æœ€æ–°å†…å®¹ï¼Œ"
            "ç„¶åç”¨ AI ç”Ÿæˆæ¯æ—¥ä¿¡æ¯ç®€æŠ¥/æ‘˜è¦ã€‚ç±»ä¼¼ OpenClaw çš„ä¿¡æ¯æµå¤„ç†ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "category": {"type": "string", "description": "åªå¤„ç†æŒ‡å®šåˆ†ç±»ï¼Œç•™ç©ºå¤„ç†å…¨éƒ¨"},
                "digest_type": {
                    "type": "string",
                    "description": "æ‘˜è¦ç±»å‹: 'briefing'(æ¯æ—¥ç®€æŠ¥), 'highlights'(ç²¾é€‰), 'analysis'(æ·±åº¦åˆ†æ)"
                },
                "max_items": {"type": "integer", "description": "çº³å…¥æ‘˜è¦çš„æœ€å¤§æ¡ç›®æ•°ï¼Œé»˜è®¤ 20"}
            },
            "required": []
        }
    }
}


@register(feed_digest_schema)
def feed_digest(category: str = "", digest_type: str = "briefing", max_items: int = 20):
    """ä¿¡æ¯æµ AI æ‘˜è¦"""
    try:
        feeds = _load_feeds()
        if not feeds:
            return "ğŸ“¡ æš‚æ— è®¢é˜…æºï¼Œè¯·å…ˆä½¿ç”¨ rss_manage æ·»åŠ è®¢é˜…ã€‚"

        max_items = min(int(max_items) if max_items else 20, 50)

        # ç­›é€‰åˆ†ç±»
        if category:
            feeds = [f for f in feeds if f.get("category", "").lower() == category.lower()]
            if not feeds:
                return f"âš ï¸ æ²¡æœ‰ '{category}' åˆ†ç±»çš„è®¢é˜…æº"

        # æŠ“å–å†…å®¹
        all_items = []
        for f in feeds:
            try:
                items = _fetch_rss(f["url"], max_items=8)
                for item in items:
                    item["source"] = f["name"]
                all_items.extend(items)
            except Exception:
                continue

        if not all_items:
            return "âš ï¸ æœªè·å–åˆ°ä»»ä½•å†…å®¹"

        # æ„å»ºå†…å®¹æ–‡æœ¬
        content_lines = []
        for i, item in enumerate(all_items[:max_items]):
            content_lines.append(f"{i+1}. [{item['source']}] {item['title']}")
            if item.get('description'):
                content_lines.append(f"   {item['description']}")
        content_text = "\n".join(content_lines)

        type_prompts = {
            "briefing": "ç”Ÿæˆä¸€ä»½ç®€æ´çš„æ¯æ—¥ä¿¡æ¯ç®€æŠ¥ï¼ŒæŒ‰è¯é¢˜å½’ç±»ï¼Œæ¯ä¸ªè¯é¢˜ 2-3 å¥è¯æ¦‚æ‹¬ã€‚",
            "highlights": "æŒ‘é€‰æœ€æœ‰ä»·å€¼çš„ 5 æ¡ä¿¡æ¯ï¼Œè¯¦ç»†ä»‹ç»æ¯æ¡çš„å†…å®¹å’Œé‡è¦æ€§ã€‚",
            "analysis": "å¯¹ä¿¡æ¯æµä¸­çš„ä¸»è¦è¶‹åŠ¿å’Œçƒ­ç‚¹è¿›è¡Œæ·±åº¦åˆ†æï¼Œç»™å‡ºè§‚å¯Ÿå’Œè§è§£ã€‚"
        }
        prompt = type_prompts.get(digest_type, type_prompts["briefing"])

        from .external_ai import call_ai
        result = call_ai(
            prompt=f"ä»¥ä¸‹æ˜¯ä»Šæ—¥ä¿¡æ¯æµå†…å®¹ ({len(all_items)} æ¡)ï¼Œè¯·{prompt}\n\n---\n{content_text}",
            provider="kimi",
            system_prompt="ä½ æ˜¯ä¿¡æ¯åˆ†æå¸ˆã€‚æ ¹æ® RSS ä¿¡æ¯æµç”Ÿæˆæ‘˜è¦ï¼Œä¿æŒå®¢è§‚å‡†ç¡®ï¼Œä¸ç¼–é€ ä¿¡æ¯ã€‚",
            temperature=0.5,
            max_tokens=4096
        )

        date_str = time.strftime("%Y-%m-%d")
        cat_str = f" [{category}]" if category else ""
        return f"ğŸ“° {date_str} ä¿¡æ¯æµæ‘˜è¦{cat_str}:\n{result}"

    except Exception as e:
        return f"âŒ ä¿¡æ¯æµæ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}"


# ==========================================
# 3. ç¤¾äº¤å¹³å°è¿æ¥å™¨ (æ¥å£é¢„ç•™)
# ==========================================
social_connector_schema = {
    "type": "function",
    "function": {
        "name": "social_connector",
        "description": (
            "ç¤¾äº¤å¹³å° API è¿æ¥å™¨ (æ¥å£é¢„ç•™)ã€‚"
            "æ”¯æŒé…ç½®å’Œç®¡ç†ç¤¾äº¤å¹³å° API è¿æ¥ï¼šå¾®åšã€Twitter/Xã€å¾®ä¿¡å…¬ä¼—å·ã€Telegram ç­‰ã€‚"
            "å½“å‰ä¸ºæ¥å£æ¡†æ¶ï¼Œéœ€è¦é…ç½®å¯¹åº”å¹³å°çš„ API Key åä½¿ç”¨ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "action": {
                    "type": "string",
                    "description": "æ“ä½œ: config(é…ç½®å¹³å°), list(åˆ—å‡ºå·²é…ç½®å¹³å°), test(æµ‹è¯•è¿æ¥), fetch(è·å–å†…å®¹)"
                },
                "platform": {
                    "type": "string",
                    "description": "å¹³å°å: weibo, twitter, wechat_mp, telegram, github, reddit"
                },
                "api_key": {"type": "string", "description": "å¹³å° API Key (config æ—¶ä½¿ç”¨)"},
                "query": {"type": "string", "description": "æŸ¥è¯¢å‚æ•° (fetch æ—¶ä½¿ç”¨)"}
            },
            "required": ["action"]
        }
    }
}

# æ”¯æŒçš„ç¤¾äº¤å¹³å°é…ç½®æ¨¡æ¿
SOCIAL_PLATFORMS = {
    "weibo": {
        "name": "å¾®åš",
        "api_key_env": "WEIBO_API_KEY",
        "base_url": "https://api.weibo.com/2",
        "endpoints": {"timeline": "/statuses/home_timeline.json", "search": "/search/topics.json"}
    },
    "twitter": {
        "name": "Twitter/X",
        "api_key_env": "TWITTER_BEARER_TOKEN",
        "base_url": "https://api.twitter.com/2",
        "endpoints": {"timeline": "/tweets/search/recent", "user": "/users/by/username"}
    },
    "wechat_mp": {
        "name": "å¾®ä¿¡å…¬ä¼—å·",
        "api_key_env": "WECHAT_MP_TOKEN",
        "base_url": "https://api.weixin.qq.com/cgi-bin",
        "endpoints": {"articles": "/material/batchget_material"}
    },
    "telegram": {
        "name": "Telegram",
        "api_key_env": "TELEGRAM_BOT_TOKEN",
        "base_url": "https://api.telegram.org",
        "endpoints": {"updates": "/getUpdates", "send": "/sendMessage"}
    },
    "github": {
        "name": "GitHub",
        "api_key_env": "GITHUB_TOKEN",
        "base_url": "https://api.github.com",
        "endpoints": {"trending": "/search/repositories", "notifications": "/notifications"}
    },
    "reddit": {
        "name": "Reddit",
        "api_key_env": "REDDIT_CLIENT_SECRET",
        "base_url": "https://oauth.reddit.com",
        "endpoints": {"hot": "/hot.json", "search": "/search.json"}
    }
}


@register(social_connector_schema)
def social_connector(action: str, platform: str = "", api_key: str = "", query: str = ""):
    """ç¤¾äº¤å¹³å°è¿æ¥å™¨"""
    try:
        _ensure_social_dir()
        meta = _load_connector_meta()

        if action == "list":
            lines = ["ğŸŒ ç¤¾äº¤å¹³å°è¿æ¥å™¨çŠ¶æ€:\n"]
            for key, info in SOCIAL_PLATFORMS.items():
                env_key = os.getenv(info["api_key_env"], "").strip()
                configured = bool(env_key)
                has_meta = key in meta
                status = "âœ… å·²é…ç½®" if configured else "â¬œ æœªé…ç½®"
                lines.append(f"  {status} {info['name']} ({key})")
                lines.append(f"       ç¯å¢ƒå˜é‡: {info['api_key_env']}")
                if has_meta and not configured:
                    lines.append("       âš ï¸ å·²è®°å½•é…ç½®ä½†ç¼ºå°‘ç¯å¢ƒå˜é‡")
            lines.append("\nğŸ’¡ é…ç½®æ–¹æ³•: åœ¨ .env ä¸­æ·»åŠ å¯¹åº” API Keyï¼›å·¥å…·ä¸ä¼šæŠŠ Key å†™å…¥æœ¬åœ°æ–‡ä»¶ã€‚")
            return "\n".join(lines)

        elif action == "config":
            if not platform:
                return "âŒ è¯·æä¾› platform"
            if platform not in SOCIAL_PLATFORMS:
                return f"âŒ ä¸æ”¯æŒçš„å¹³å°: {platform}ã€‚æ”¯æŒ: {', '.join(SOCIAL_PLATFORMS.keys())}"

            info = SOCIAL_PLATFORMS[platform]
            runtime_note = ""
            if api_key:
                os.environ[info["api_key_env"]] = api_key
                runtime_note = "\nâš ï¸ å·²å†™å…¥å½“å‰è¿›ç¨‹ç¯å¢ƒå˜é‡ï¼ˆä¸´æ—¶ï¼‰ï¼Œé‡å¯åä¼šå¤±æ•ˆã€‚"

            meta[platform] = {
                "configured_at": time.strftime("%Y-%m-%d %H:%M"),
                "api_key_env": info["api_key_env"],
            }
            _save_connector_meta(meta)
            return (
                f"âœ… å·²è®°å½• {info['name']} å¹³å°é…ç½®ï¼ˆä¸ä¿å­˜æ˜æ–‡ API Keyï¼‰ã€‚"
                f"\nè¯·åœ¨ .env ä¸­è®¾ç½®: {info['api_key_env']}=..."
                f"{runtime_note}"
            )

        elif action == "test":
            if not platform:
                return "âŒ è¯·æä¾› platform"
            if platform not in SOCIAL_PLATFORMS:
                return f"âŒ ä¸æ”¯æŒçš„å¹³å°: {platform}"

            info = SOCIAL_PLATFORMS[platform]
            key = _get_platform_api_key(platform, api_key)
            if not key:
                return f"âŒ {info['name']} æœªé…ç½® API Key"
            return f"âœ… {info['name']} API Key å·²é…ç½® (é•¿åº¦: {len(key)})\nâš ï¸ å…·ä½“è¿é€šæ€§æµ‹è¯•éœ€è¦ç½‘ç»œè¯·æ±‚ï¼Œè¯·ä½¿ç”¨ fetch æ“ä½œéªŒè¯ã€‚"

        elif action == "fetch":
            if not platform:
                return "âŒ è¯·æä¾› platform"
            if platform not in SOCIAL_PLATFORMS:
                return f"âŒ ä¸æ”¯æŒçš„å¹³å°: {platform}"

            info = SOCIAL_PLATFORMS[platform]
            key = _get_platform_api_key(platform, api_key)
            if not key:
                return f"âŒ {info['name']} æœªé…ç½® API Keyã€‚è¯·å…ˆé…ç½®ã€‚"

            # GitHub ç‰¹æ®Šå¤„ç† (æœ€å¸¸ç”¨)
            if platform == "github":
                return _fetch_github(key, query)

            return (
                f"âš ï¸ {info['name']} å†…å®¹è·å–æ¥å£å¼€å‘ä¸­ã€‚\n"
                f"  API ç«¯ç‚¹: {info['base_url']}\n"
                f"  å¯ç”¨ç«¯ç‚¹: {json.dumps(info['endpoints'], ensure_ascii=False)}\n"
                f"  ğŸ’¡ å¯é€šè¿‡ fetch_url ç›´æ¥è®¿é—® API ç«¯ç‚¹è·å–æ•°æ®ã€‚"
            )

        else:
            return f"âŒ æœªçŸ¥æ“ä½œ: {action}ã€‚æ”¯æŒ: config, list, test, fetch"

    except Exception as e:
        return f"âŒ ç¤¾äº¤è¿æ¥å™¨å¤±è´¥: {e}"


# ==========================================
# 4. ç»Ÿä¸€ä¿¡æ¯æµå¤„ç†ç®¡é“
# fetch -> dedupe -> sort -> summarize -> taskify
# ==========================================
infoflow_pipeline_schema = {
    "type": "function",
    "function": {
        "name": "infoflow_pipeline",
        "description": (
            "ç»Ÿä¸€å¤„ç†ä¿¡æ¯æµï¼šæŠ“å– RSSã€å»é‡ã€æ’åºã€AI æ‘˜è¦ï¼Œå¹¶å¯é€‰ç”Ÿæˆå¾…åŠä»»åŠ¡ã€‚"
            "é€‚åˆæ¯æ—¥ä¿¡æ¯æ‘„å–å’Œè¡ŒåŠ¨åŒ–ã€‚"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "category": {"type": "string", "description": "åˆ†ç±»è¿‡æ»¤ï¼Œç•™ç©ºè¡¨ç¤ºå…¨éƒ¨"},
                "max_items": {"type": "integer", "description": "å¤„ç†åçš„æœ€å¤§æ¡æ•°ï¼Œé»˜è®¤ 20"},
                "per_feed_limit": {"type": "integer", "description": "æ¯ä¸ªè®¢é˜…æºæŠ“å–æ¡æ•°ï¼Œé»˜è®¤ 8"},
                "digest_type": {
                    "type": "string",
                    "description": "æ‘˜è¦ç±»å‹: briefing/highlights/analysisï¼Œé»˜è®¤ briefing"
                },
                "taskify": {"type": "boolean", "description": "æ˜¯å¦ç”Ÿæˆä»»åŠ¡å»ºè®®ï¼Œé»˜è®¤ true"},
                "create_todos": {"type": "boolean", "description": "æ˜¯å¦å†™å…¥å¾…åŠåˆ—è¡¨ï¼Œé»˜è®¤ false"},
                "task_limit": {"type": "integer", "description": "æœ€å¤šç”Ÿæˆä»»åŠ¡æ•°ï¼Œé»˜è®¤ 5"}
            },
            "required": []
        }
    }
}


def _item_time_score(item):
    date_text = (item.get("date") or "").strip()
    if not date_text:
        return 0.0

    try:
        return parsedate_to_datetime(date_text).timestamp()
    except Exception:
        pass

    for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%Y-%m-%d", "%Y/%m/%d %H:%M", "%Y/%m/%d"):
        try:
            return datetime.strptime(date_text, fmt).timestamp()
        except Exception:
            continue

    return 0.0


def _item_dedupe_key(item):
    link = (item.get("link") or "").strip().lower()
    if link:
        return f"link:{link}"
    title = re.sub(r"\s+", " ", (item.get("title") or "").strip().lower())
    source = (item.get("source") or "").strip().lower()
    return f"title:{source}:{title}"


def _heuristic_tasks(items, limit: int):
    tasks = []
    seen = set()
    for item in items:
        title = (item.get("title") or "").strip()
        source = (item.get("source") or "").strip()
        if not title:
            continue

        task = f"é˜…è¯»å¹¶è·Ÿè¿›: {title}"
        if source:
            task += f" [{source}]"

        if task in seen:
            continue
        seen.add(task)
        tasks.append(task)
        if len(tasks) >= limit:
            break
    return tasks


@register(infoflow_pipeline_schema)
def infoflow_pipeline(
    category: str = "",
    max_items: int = 20,
    per_feed_limit: int = 8,
    digest_type: str = "briefing",
    taskify: bool = True,
    create_todos: bool = False,
    task_limit: int = 5,
):
    try:
        feeds = _load_feeds()
        if not feeds:
            return "ğŸ“¡ æš‚æ— è®¢é˜…æºï¼Œè¯·å…ˆä½¿ç”¨ rss_manage æ·»åŠ è®¢é˜…ã€‚"

        max_items = max(1, min(int(max_items) if max_items else 20, 50))
        per_feed_limit = max(1, min(int(per_feed_limit) if per_feed_limit else 8, 20))
        task_limit = max(1, min(int(task_limit) if task_limit else 5, 20))

        if category:
            feeds = [f for f in feeds if f.get("category", "").lower() == category.lower()]
            if not feeds:
                return f"âš ï¸ æ²¡æœ‰ '{category}' åˆ†ç±»çš„è®¢é˜…æº"

        all_items = []
        fetch_errors = []
        for feed in feeds:
            try:
                items = _fetch_rss(feed["url"], max_items=per_feed_limit)
                for item in items:
                    item["source"] = feed["name"]
                    item["category"] = feed.get("category", "")
                all_items.extend(items)
            except Exception as e:
                fetch_errors.append(f"{feed['name']}: {e}")

        if not all_items:
            err_msg = "ï¼›".join(fetch_errors) if fetch_errors else "æœªçŸ¥é”™è¯¯"
            return f"âš ï¸ æœªè·å–åˆ°ä»»ä½•å†…å®¹ã€‚{err_msg}"

        # dedupe: å¯¹åŒä¸€ key ä»…ä¿ç•™è¾ƒæ–°çš„æ¡ç›®
        deduped = {}
        for item in all_items:
            key = _item_dedupe_key(item)
            if key not in deduped:
                deduped[key] = item
                continue
            if _item_time_score(item) >= _item_time_score(deduped[key]):
                deduped[key] = item

        sorted_items = sorted(deduped.values(), key=_item_time_score, reverse=True)
        selected = sorted_items[:max_items]

        content_lines = []
        for i, item in enumerate(selected):
            content_lines.append(f"{i+1}. [{item.get('source', '')}] {item.get('title', '')}")
            if item.get("description"):
                content_lines.append(f"   {item['description']}")
            if item.get("link"):
                content_lines.append(f"   é“¾æ¥: {item['link']}")
            if item.get("date"):
                content_lines.append(f"   æ—¶é—´: {item['date']}")
        content_text = "\n".join(content_lines)

        type_prompts = {
            "briefing": "ç”Ÿæˆä¸€ä»½ç®€æ´çš„æ¯æ—¥ä¿¡æ¯ç®€æŠ¥ï¼ŒæŒ‰è¯é¢˜å½’ç±»ï¼Œæ¯ä¸ªè¯é¢˜ 2-3 å¥è¯æ¦‚æ‹¬ã€‚",
            "highlights": "æŒ‘é€‰æœ€æœ‰ä»·å€¼çš„ 5 æ¡ä¿¡æ¯ï¼Œè¯¦ç»†ä»‹ç»æ¯æ¡çš„å†…å®¹å’Œé‡è¦æ€§ã€‚",
            "analysis": "å¯¹ä¿¡æ¯æµä¸­çš„ä¸»è¦è¶‹åŠ¿å’Œçƒ­ç‚¹è¿›è¡Œæ·±åº¦åˆ†æï¼Œç»™å‡ºè§‚å¯Ÿå’Œè§è§£ã€‚",
        }
        digest_prompt = type_prompts.get((digest_type or "briefing").lower(), type_prompts["briefing"])

        from .external_ai import call_ai
        digest_result = call_ai(
            prompt=f"ä»¥ä¸‹æ˜¯ä»Šæ—¥ä¿¡æ¯æµå†…å®¹ï¼Œè¯·{digest_prompt}\n\n---\n{content_text}",
            provider="kimi",
            system_prompt="ä½ æ˜¯ä¿¡æ¯åˆ†æå¸ˆã€‚æ ¹æ®è¾“å…¥å†…å®¹ç”Ÿæˆæ‘˜è¦ï¼Œä¸è¦ç¼–é€ ã€‚",
            temperature=0.4,
            max_tokens=4096,
        )

        lines = [
            f"ğŸ§  ä¿¡æ¯æµå¤„ç†å®Œæˆï¼šåŸå§‹ {len(all_items)} æ¡ â†’ å»é‡å {len(deduped)} æ¡ â†’ è¾“å‡º {len(selected)} æ¡ã€‚",
            "",
            "ğŸ“° æ‘˜è¦ï¼š",
            digest_result,
            "",
            "ğŸ“Œ å¤´æ¡é¢„è§ˆï¼š",
        ]

        for i, item in enumerate(selected[:min(len(selected), 10)]):
            lines.append(f"  {i+1}. [{item.get('source', '')}] {item.get('title', '')}")

        if fetch_errors:
            lines.append("")
            lines.append(f"âš ï¸ æŠ“å–å¤±è´¥ {len(fetch_errors)} ä¸ªæºï¼š")
            lines.extend([f"  - {x}" for x in fetch_errors[:8]])

        if taskify:
            tasks = _heuristic_tasks(selected, task_limit)
            lines.append("")
            lines.append("âœ… ä»»åŠ¡å»ºè®®ï¼š")
            for i, task in enumerate(tasks):
                lines.append(f"  {i+1}. {task}")

            if create_todos and tasks:
                from .daily_tools import todo_manage

                created = 0
                todo_errors = []
                for task in tasks:
                    resp = todo_manage(
                        action="add",
                        content=task,
                        priority="medium",
                        category=category or "ä¿¡æ¯æµ",
                    )
                    if isinstance(resp, str) and resp.startswith("âœ…"):
                        created += 1
                    else:
                        todo_errors.append(str(resp))

                lines.append(f"\nğŸ—‚ï¸ å·²å†™å…¥å¾…åŠ: {created}/{len(tasks)}")
                if todo_errors:
                    lines.append("âš ï¸ å¾…åŠå†™å…¥å¼‚å¸¸ï¼š")
                    lines.extend([f"  - {e}" for e in todo_errors[:5]])

        return "\n".join(lines)

    except Exception as e:
        return f"âŒ ä¿¡æ¯æµå¤„ç†å¤±è´¥: {e}"


def _fetch_github(token: str, query: str = ""):
    """GitHub API æ•°æ®è·å–"""
    try:
        import urllib.request
        import urllib.parse

        if query:
            # æœç´¢ä»“åº“
            url = f"https://api.github.com/search/repositories?q={urllib.parse.quote(query)}&sort=stars&per_page=10"
        else:
            # è·å–é€šçŸ¥
            url = "https://api.github.com/notifications?per_page=10"

        req = urllib.request.Request(url, headers={
            "Authorization": f"Bearer {token}",
            "Accept": "application/vnd.github+json",
            "User-Agent": "AI-Assistant"
        })

        with urllib.request.urlopen(req, timeout=10) as resp:
            data = json.loads(resp.read().decode())

        if query:
            items = data.get("items", [])
            if not items:
                return f"ğŸ” GitHub æœªæ‰¾åˆ°ç›¸å…³ä»“åº“: {query}"
            lines = [f"ğŸ” GitHub æœç´¢ '{query}' ({data.get('total_count', 0)} ç»“æœ):\n"]
            for r in items[:10]:
                stars = r.get("stargazers_count", 0)
                desc = r.get("description", "") or ""
                lines.append(f"  â­ {stars:,} {r['full_name']}")
                if desc:
                    lines.append(f"       {desc[:80]}")
                lines.append(f"       ğŸ”— {r['html_url']}")
            return "\n".join(lines)
        else:
            if not data:
                return "ğŸ“¬ GitHub æš‚æ— æœªè¯»é€šçŸ¥"
            lines = [f"ğŸ“¬ GitHub é€šçŸ¥ ({len(data)} æ¡):\n"]
            for n in data[:10]:
                subject = n.get("subject", {})
                lines.append(f"  [{n.get('reason', '')}] {subject.get('title', '')}")
                lines.append(f"       {subject.get('type', '')} - {n.get('repository', {}).get('full_name', '')}")
            return "\n".join(lines)

    except Exception as e:
        return f"âŒ GitHub API è¯·æ±‚å¤±è´¥: {e}"


# ==========================================
# 5. å¾®ä¿¡å…¬ä¼—å· RSS æ¡¥æ¥
# ==========================================
WECHAT_RSS_BRIDGES = {
    "werss": {
        "name": "WeRSS",
        "url_template": "https://werss.app/api/v1/feeds/{account_id}.xml",
        "api_key_env": "WERSS_API_KEY",
        "help_url": "https://werss.app",
        "description": "ä»˜è´¹æœåŠ¡ï¼Œç¨³å®šå¯é ",
    },
    "feeddd": {
        "name": "Feeddd",
        "url_template": "https://feeddd.org/feeds/{account_id}",
        "api_key_env": "",
        "help_url": "https://feeddd.org",
        "description": "å…è´¹ç¤¾åŒºé¡¹ç›®",
    },
    "wechat2rss": {
        "name": "WeChat2RSS",
        "url_template": "https://wechat2rss.xlab.app/feed/{account_id}.xml",
        "api_key_env": "WECHAT2RSS_TOKEN",
        "help_url": "https://wechat2rss.xlab.app",
        "description": "å¼€æºé¡¹ç›®ï¼Œå¯è‡ªå»º",
    },
    "custom": {
        "name": "è‡ªå®šä¹‰ RSS æº",
        "url_template": "{custom_url}",
        "api_key_env": "",
        "help_url": "",
        "description": "è‡ªè¡Œæä¾›å®Œæ•´ RSS URL",
    },
}

wechat_bridge_schema = {
    "type": "function",
    "function": {
        "name": "wechat_bridge",
        "description": (
            "å¾®ä¿¡å…¬ä¼—å· RSS æ¡¥æ¥å·¥å…·ã€‚é€šè¿‡ç¬¬ä¸‰æ–¹ RSS æœåŠ¡è®¢é˜…å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œ"
            "æ— éœ€å¾®ä¿¡ API æˆ–å®¢æˆ·ç«¯è‡ªåŠ¨åŒ–ï¼Œå®‰å…¨æ— å°å·é£é™©ã€‚"
            "æ”¯æŒ: subscribe(è®¢é˜…), list(åˆ—å‡º), fetch(æŠ“å–), bridges(æŸ¥çœ‹æœåŠ¡å•†)"
        ),
        "parameters": {
            "type": "object",
            "properties": {
                "action": {
                    "type": "string",
                    "description": "æ“ä½œ: subscribe(è®¢é˜…å…¬ä¼—å·), unsubscribe(å–æ¶ˆ), list(åˆ—å‡º), fetch(æŠ“å–), fetch_all(å…¨éƒ¨æŠ“å–), bridges(æœåŠ¡å•†åˆ—è¡¨)",
                },
                "account_name": {"type": "string", "description": "å…¬ä¼—å·åç§°ï¼ˆsubscribe/unsubscribe æ—¶ä½¿ç”¨ï¼‰"},
                "account_id": {
                    "type": "string",
                    "description": "å…¬ä¼—å· ID æˆ–å¾®ä¿¡å·ï¼ˆç”¨äºç”Ÿæˆ RSS URLï¼Œå„æ¡¥æ¥æœåŠ¡å®šä¹‰ä¸åŒï¼‰",
                },
                "bridge": {
                    "type": "string",
                    "description": "RSS æ¡¥æ¥æœåŠ¡: werss/feeddd/wechat2rss/customï¼Œé»˜è®¤ feeddd",
                },
                "custom_url": {"type": "string", "description": "è‡ªå®šä¹‰ RSS URLï¼ˆbridge=custom æ—¶ä½¿ç”¨ï¼‰"},
                "max_items": {"type": "integer", "description": "æœ€å¤§æŠ“å–æ¡ç›®æ•°ï¼Œé»˜è®¤ 10"},
            },
            "required": ["action"],
        },
    },
}


def _load_wechat_subs():
    subs_obj = _guarded_social_file("wechat_subs.json")
    if subs_obj.exists():
        with open(subs_obj, 'r', encoding='utf-8') as f:
            data = json.load(f)
            if isinstance(data, list):
                return data
    return []


def _save_wechat_subs(subs):
    subs_obj = _guarded_social_file("wechat_subs.json")
    with open(subs_obj, 'w', encoding='utf-8') as f:
        json.dump(subs, f, ensure_ascii=False, indent=2)


def _build_wechat_rss_url(bridge: str, account_id: str, custom_url: str = ""):
    """Build the RSS URL from bridge template."""
    bridge_info = WECHAT_RSS_BRIDGES.get(bridge)
    if not bridge_info:
        return None, f"âŒ æœªçŸ¥æ¡¥æ¥æœåŠ¡: {bridge}ã€‚å¯é€‰: {', '.join(WECHAT_RSS_BRIDGES.keys())}"

    if bridge == "custom":
        if not custom_url:
            return None, "âŒ bridge=custom æ—¶å¿…é¡»æä¾› custom_url"
        url = bridge_info["url_template"].format(custom_url=custom_url)
    else:
        if not account_id:
            return None, "âŒ è¯·æä¾› account_id (å…¬ä¼—å· ID æˆ–å¾®ä¿¡å·)"
        url = bridge_info["url_template"].format(account_id=account_id)

    # Append API key if needed
    api_key_env = bridge_info.get("api_key_env", "")
    if api_key_env:
        api_key = os.getenv(api_key_env, "").strip()
        if api_key:
            sep = "&" if "?" in url else "?"
            url = f"{url}{sep}token={api_key}"

    return url, None


@register(wechat_bridge_schema)
def wechat_bridge(
    action: str,
    account_name: str = "",
    account_id: str = "",
    bridge: str = "feeddd",
    custom_url: str = "",
    max_items: int = 10,
):
    """å¾®ä¿¡å…¬ä¼—å· RSS æ¡¥æ¥"""
    try:
        action = (action or "").strip().lower()
        max_items = max(1, min(int(max_items) if max_items else 10, 30))

        if action == "bridges":
            lines = ["ğŸ”Œ å¾®ä¿¡å…¬ä¼—å· RSS æ¡¥æ¥æœåŠ¡:\n"]
            for bid, binfo in WECHAT_RSS_BRIDGES.items():
                key_status = ""
                if binfo.get("api_key_env"):
                    has_key = bool(os.getenv(binfo["api_key_env"], "").strip())
                    key_status = " âœ…" if has_key else f" âš ï¸ éœ€é…ç½® {binfo['api_key_env']}"
                lines.append(f"  [{bid}] {binfo['name']}{key_status}")
                lines.append(f"    {binfo['description']}")
                if binfo.get("help_url"):
                    lines.append(f"    ğŸ”— {binfo['help_url']}")
                lines.append("")
            lines.append(
                "ğŸ’¡ æ¨è: feeddd (å…è´¹) æˆ– werss (ä»˜è´¹ä½†ç¨³å®š)ã€‚\n"
                "   ä¹Ÿå¯ bridge=custom ç›´æ¥ä¼ å…¥ä»»ä½• RSS URLã€‚"
            )
            return "\n".join(lines)

        subs = _load_wechat_subs()

        if action == "subscribe":
            if not account_name:
                return "âŒ è¯·æä¾› account_name (å…¬ä¼—å·åç§°)"

            bridge = (bridge or "feeddd").strip().lower()
            rss_url, url_err = _build_wechat_rss_url(bridge, account_id, custom_url)
            if url_err:
                return url_err

            # Check duplicate
            for s in subs:
                if s.get("account_name") == account_name or s.get("rss_url") == rss_url:
                    return f"âš ï¸ å·²è®¢é˜…: {account_name}"

            sub = {
                "account_name": account_name,
                "account_id": account_id or "",
                "bridge": bridge,
                "rss_url": rss_url,
                "added_at": time.strftime("%Y-%m-%d %H:%M"),
            }
            subs.append(sub)
            _save_wechat_subs(subs)

            # Also add to main RSS feeds for unified pipeline
            feeds = _load_feeds()
            if not any(f["url"] == rss_url for f in feeds):
                feeds.append({
                    "url": rss_url,
                    "name": f"[å¾®ä¿¡] {account_name}",
                    "category": "wechat",
                    "added": time.strftime("%Y-%m-%d %H:%M"),
                })
                _save_feeds(feeds)

            return (
                f"âœ… å·²è®¢é˜…å¾®ä¿¡å…¬ä¼—å·: {account_name}\n"
                f"  æ¡¥æ¥æœåŠ¡: {WECHAT_RSS_BRIDGES.get(bridge, {}).get('name', bridge)}\n"
                f"  RSS URL: {rss_url}\n"
                f"  ğŸ’¡ å·²åŒæ­¥åˆ° RSS è®¢é˜…ï¼Œå¯é€šè¿‡ rss_manage/infoflow_pipeline ç»Ÿä¸€å¤„ç†ã€‚"
            )

        elif action == "unsubscribe":
            if not account_name:
                return "âŒ è¯·æä¾› account_name"
            before = len(subs)
            removed_urls = [s["rss_url"] for s in subs if s.get("account_name") == account_name]
            subs = [s for s in subs if s.get("account_name") != account_name]
            if len(subs) == before:
                return f"âŒ æœªæ‰¾åˆ°è®¢é˜…: {account_name}"
            _save_wechat_subs(subs)

            # Also remove from main RSS feeds
            if removed_urls:
                feeds = _load_feeds()
                feeds = [f for f in feeds if f["url"] not in removed_urls]
                _save_feeds(feeds)

            return f"âœ… å·²å–æ¶ˆè®¢é˜…: {account_name}"

        elif action == "list":
            if not subs:
                return (
                    "ğŸ“± æš‚æ— å¾®ä¿¡å…¬ä¼—å·è®¢é˜…\n"
                    "ğŸ’¡ ä½¿ç”¨ wechat_bridge(action='bridges') æŸ¥çœ‹å¯ç”¨æ¡¥æ¥æœåŠ¡\n"
                    "   ä½¿ç”¨ wechat_bridge(action='subscribe', ...) æ·»åŠ è®¢é˜…"
                )
            lines = [f"ğŸ“± å¾®ä¿¡å…¬ä¼—å·è®¢é˜… ({len(subs)} ä¸ª):\n"]
            for s in subs:
                bridge_name = WECHAT_RSS_BRIDGES.get(s.get("bridge", ""), {}).get("name", s.get("bridge", "?"))
                lines.append(f"  ğŸ“° {s['account_name']}")
                lines.append(f"     æ¡¥æ¥: {bridge_name}")
                lines.append(f"     RSS: {s['rss_url']}")
                lines.append("")
            return "\n".join(lines)

        elif action == "fetch":
            if not account_name:
                return "âŒ è¯·æä¾› account_name"
            sub = next((s for s in subs if s.get("account_name") == account_name), None)
            if not sub:
                return f"âŒ æœªæ‰¾åˆ°è®¢é˜…: {account_name}"

            url = sub["rss_url"]
            url_err = _validate_public_http_url(url)
            if url_err:
                return url_err

            items = _fetch_rss(url, max_items)
            if not items:
                return f"âš ï¸ æœªæŠ“å–åˆ°å†…å®¹: {account_name}\n  RSS: {url}\n  ğŸ’¡ å¯èƒ½æ¡¥æ¥æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"

            lines = [f"ğŸ“± [{account_name}] æœ€æ–°æ–‡ç«  ({len(items)} ç¯‡):\n"]
            for i, item in enumerate(items):
                lines.append(f"  {i+1}. **{item['title']}**")
                if item.get("date"):
                    lines.append(f"     ğŸ“… {item['date']}")
                if item.get("link"):
                    lines.append(f"     ğŸ”— {item['link']}")
                if item.get("description"):
                    lines.append(f"     {item['description']}")
                lines.append("")
            return "\n".join(lines)

        elif action == "fetch_all":
            if not subs:
                return "ğŸ“± æš‚æ— å¾®ä¿¡å…¬ä¼—å·è®¢é˜…"

            all_items = []
            errors = []
            for s in subs:
                try:
                    url = s["rss_url"]
                    url_err = _validate_public_http_url(url)
                    if url_err:
                        errors.append(f"{s['account_name']}: {url_err}")
                        continue
                    items = _fetch_rss(url, max_items=5)
                    for item in items:
                        item["source"] = f"[å¾®ä¿¡] {s['account_name']}"
                    all_items.extend(items)
                except Exception as e:
                    errors.append(f"{s['account_name']}: {e}")

            if not all_items:
                err_msg = "; ".join(errors) if errors else "æœªçŸ¥é”™è¯¯"
                return f"âš ï¸ æœªæŠ“å–åˆ°ä»»ä½•å¾®ä¿¡æ–‡ç« ã€‚{err_msg}"

            lines = [f"ğŸ“± å¾®ä¿¡å…¬ä¼—å·æ±‡æ€» ({len(all_items)} ç¯‡ï¼Œæ¥è‡ª {len(subs)} ä¸ªå·):\n"]
            for i, item in enumerate(all_items[:max_items]):
                lines.append(f"  {i+1}. [{item.get('source', '')}] **{item['title']}**")
                if item.get("link"):
                    lines.append(f"     ğŸ”— {item['link']}")
                if item.get("description"):
                    lines.append(f"     {item['description']}")
                lines.append("")

            if errors:
                lines.append(f"âš ï¸ {len(errors)} ä¸ªå·æŠ“å–å¤±è´¥: {'; '.join(errors[:5])}")

            return "\n".join(lines)

        else:
            return f"âŒ æœªçŸ¥æ“ä½œ: {action}ã€‚æ”¯æŒ: subscribe, unsubscribe, list, fetch, fetch_all, bridges"

    except Exception as e:
        return f"âŒ å¾®ä¿¡æ¡¥æ¥å¤±è´¥: {e}"
